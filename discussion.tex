
In this paper, we introduced a novel approach based on example forgetting to build more robust models for a natural language inference task. We  finetuned a pre-trained model on a set of ``hard'' examples selected by measuring ``example forgetting''~\citep{toneva2018empirical}. We evaluated the robustness of our approach by training exclusively using the MNLI dataset and the evaluating the model on the out-of-distribution test set of HANS~\citep{linzen2019right}. We improve \bertbase and \bertlarge performance on the challenging HANS test set by more than 15\% and 5\%, respectively. Although this paper focused on natural language inference, the method is widely applicable in other tasks and contexts. This constitutes one possible direction for future work. Moreover, we plan to analyze the forgettable examples more thoroughly to understand their special properties. Finally, we will study smoother re-weighting of the training examples and re-interpret the studied approach more formally in the context of importance weighted empirical risk minimization~\citep{sugiyama2007covariate}.

% \item investigate differences between hard examples and most forgotten examples 
%     \item smoothly integrate the finetuning phase by softly reweighting examples
%     \item compute more explicit measures of correlation between forgettables and the biases
%     \item understand why BERT forgettables suck.
%     \item can we even learn to avoid biases if no example contain counter examples of those biases ? put this in discussion
% \end{itemize}
