% QQP (Quora question paraphrase) is a paraphrase detection dataset from Quora questions. 
% Each question pair is labeled by either positive or negative. 
% Performance of strong NLP models is very high on the evaluation sets from this data. 
% Recently, PAWS is introduced \cite{zhang-etal-2019-paws} which is made using back translation and applying heuristics for cleanup. The word overlap in PAWS examples is significantly higher than QQP making it a challenging evaluation set for QQP models.
% For example, the accuracy of BERT trained on QQP is drops from around 91\% to 32\%.

In this section, we report the results of our method applied to QQP (see Section \ref{sec:dataset_qqp}). We assess the robustness of our fine-tuned models on PAWS. Results can be found in Table \ref{tab:paws}. Considering AUC as the suitable metric, we observe an improvement for all the target models by fine-tuning on forgettables. The improvement is the clearest for \xlnetlarge where the AUC increases by more than 6 points. XLNET is overall more robust than BERT. While performance degrades slightly on QQP after fine-tuning, on average between QQP and PAWS, the fine-tuned models perform better than the initial ones. Those conclusions remain true when considering accuracy. Note that the achieved improvement is despite the fact that the ground-truth labels in QQP contain noisy annotation~\citep{iyer2017first} and a portion of performance loss could be attributed to it.

\begin{table*}[ht]
\small
\centering
\begin{tabular}{llcccccc}
\toprule
& \textbf{Model}                           & \multicolumn{2}{c}{\textbf{QQP}} & \multicolumn{2}{c}{\textbf{PAWS}} &           \multicolumn{2}{c}{\textbf{Avg.}}         \\
                                && Acc           & AUC          & Acc           & AUC          & Acc & AUC \\
%\midrule
%Random (p=0.3624)               & 50    & 58.96 & 56.60 & 41.77 & 53.30 & 50.365 \\
%Random (p=0.5)                  & 49.79 & 62.23 & 50.52 & 46.67 & 50.16 & 54.45 \\
%Random (p=0.6376)               & 50    & 66.04 & 43.40 & 50.68 & 46.70 & 58.36 \\
\midrule
\small{1}&\fbow                  & 73.0 & 80.3 & 61.6 & 33.4 & 67.3 & 56.9 \\
\small{2}&\flstm                 & 72.3 & 81.0 & 54.1 & 33.8 & 62.2 & 57.4 \\
%&\fbert                 & 54.5 & 56.8 & 42.8 & 22.9 & 48.6 & 39.9 \\
\midrule
\small{3}& BERT                       & 90.6 & 96.6 & 32.2 & 33.8 & 61.4 & 65.2 \\
\small{4}& BERT + \fbow    & 88.8 & 95.4 & 44.8 & 36.4 & 66.8 & 65.9 \\
\small{5}& BERT + \flstm & 88.2 & 95.2 & 41.9 & 37.2 & 65.1 & 66.2 \\
% BERT + BERT forgettables   & 87.7 & 95.2 & 48.4 & 34.4 & 68.1 & 64.8 \\
\midrule
\small{6}& \bertlarge                       & 91.1 & \textbf{96.9} & 34.6 & 35.1 & 62.9 & 66.0 \\
\small{7}& \bertlarge + \fbow & 87.5 & 93.4 & 54.6 & 35.9 & 71.3 & 66.2  \\
\small{8}& \bertlarge + \flstm & 86.3 & 93.5 & 55.2 & 35.8 & 71.3 & 65.0 \\
% \bertlarge + BERT forgettables & 83.0 & 92.6 &  \\
\midrule
\small{9}& XLNET   & 90.9 & 96.6 & 36.6 & 37.5 & 63.8 & 67.1 \\
\small{10}& XLNET + \fbow    & 88.5 & 95.1 & 52.9 & 39.3 & 70.1 & 67.2 \\
\small{11}& XLNET + \flstm & 87.9 & 95.0 & 49.5 & 40.5 & 68.7 & 67.8 \\
% XLNET + BERT forgettables   & 88.6 & 95.2 & 60.4 & 38.1 & 74.5 & 66.6 \\
\midrule
\small{12}& \xlnetlarge                       & \textbf{91.6} & \textbf{97.0}   & 49.6 & 43.4 & 70.6 & 70.2 \\
\small{13}& \xlnetlarge + \fbow    & 86.7 & 93.0 & \textbf{68.9} & \textbf{50.1} & \textbf{78.1} & \textbf{71.6}\\
\small{14}& \xlnetlarge + \flstm & 85.1 & 92.8 & 63.7 & 46.8 & 74.4 & 69.8\\
% \XLNETlarge + BERT forgettables & 86.1 & 92.5 & 69.3 & 45.1 & 77.8 & 69.0\\
\midrule
% FROM PAWS PAPER, TABLE 7:
&\emph{From~\citet{zhang-etal-2019-paws}} \\
% BOW                                          & 83.2 & 89.5 & 29.0 & 27.1 & 56.1 & 58.3 \\
% BiLSTM                                       & 86.3 & 91.6 & 34.8 & 37.9 & 60.6 & 64.8 \\
% ESIM~\citep{chen-etal-2017-enhanced}         & 85.3 & 92.8 & 38.9 & 26.9 & 62.1 & 59.9 \\
% DecAtt~\citep{parikh-etal-2016-decomposable} & 87.8 & 93.9 & 33.3 & 26.3 & 60.6 & 60.1 \\
% DIIN~\citep{gong2017natural}                 & 89.2 & 95.2 & 32.8 & 32.4 & 61.0 & 63.8 \\
\small{15}& BERT                & 90.5 & 96.3 & 33.5 & 35.1 & 62.0 & 65.7 \\
\bottomrule
\end{tabular}
\caption{Results of the \emph{base} and \emph{large} versions of various recent NLU models trained on different sources of training examples. Accuracy (\%) and AUC score (\%) of precision-recall curves are reported for QQP test set and PAWS development set alongside their averages. Lines 1 and 2 represent BERT trained solely on \fbow and \flstm from random initialization. Line 15 is to compare to our BERT baseline. Rest are fine-tuned on QQP and, if indicated, additional stage of fine-tuning is performed. Bold numbers represent highest number in the column.
% \yado{Summary of results: Considering AUC as the metric suitable for PAWS (i) we improve all main models by fine-tuning on the forgettables. The improvement is more clear for \XLNETlarge model where AUC increase more than 6 point; (ii) XLNET is more robust than
%  BERT. (iii)  We lose slightly on QQP after finetuning
%  but on the average of QQP and HANS the models finetuned on forgettables are better than their initial models. }
 }
\label{tab:paws}
\end{table*}