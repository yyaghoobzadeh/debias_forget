\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{latexsym}
\usepackage{subcaption}
\usepackage{url}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{multirow}

\newcounter{notecounter}
\newcommand{\enotesoff}{\long\gdef\enote##1##2{}}
\newcommand{\enoteson}{\long\gdef\enote##1##2{{
\stepcounter{notecounter}
{\large\bf
\hspace{1cm}\arabic{notecounter} $<<<$ ##1: ##2
$>>>$\hspace{1cm}}}}}
\enoteson

\newcommand\alex[1]{\textcolor{red}{Aless: #1}}
\newcommand\remi[1]{\textcolor{green}{Remi: #1}}
\newcommand\yado[1]{\textcolor{orange}{Yado: #1}}

% \input macros

\def\balancedbert{17,748}
\def\balancedlstm{46,740}
\def\balancedbow{63,390}

\title{Robust Language Understanding Models \\ with Example Forgetting}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Yadollah Yaghoobzadeh, Remi Tachet, T.J. Hazen, Alessandro Sordoni \\
  Microsoft Research \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
We investigate whether example forgetting, a recently introduced measure of hardness of examples, can be used to select training examples in order to increase robustness of natural language understanding models. We analyze forgetting events for natural language inference task and provide evidence that examples that are most forgotten under simpler NLU models can be used to increase robustness of larger models such as the recently proposed BERT.  
\end{abstract}

\section{Introduction}
\input{intro}

\section{Methodology}
\input{method}

\section{Evaluation}
\input{results}


\iffalse
\section{Analysis}
\input{analysis}
\fi


\section{Discussion and Conclusion}
\input{discussion}

\bibliography{bibliography}
\bibliographystyle{acl_natbib}

\appendix
\section{Detailed results on HANS}
\label{sec:detailedresults}
\input{hans_detailed_results.tex}

\end{document}

%TODO:
%- https://dawn.cs.stanford.edu/2019/03/22/glue/ (Signal 4: Dataset Slicing) 

% https://microsoft-my.sharepoint.com/:x:/r/personal/yayaghoo_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7B41975454-abff-488b-8f55-c550ad840d9f%7D&action=edit&activeCell=%27compare_baseModels%27!D9&wdInitialSession=c219a303-a524-465a-bdd6-28ac21d1312c&wdRldC=1

% \documentclass{article} % For LaTeX2e
% \usepackage{iclr2019_conference,times}

% % Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

% \usepackage{hyperref}
% \hypersetup{
%     colorlinks=true,
%     citecolor=black,
%     linkcolor=black,
%     filecolor=magenta,      
%     urlcolor=cyan,
% }
% \usepackage{url}
% \usepackage{caption}    
% \usepackage{wrapfig}
% \usepackage{subfig}
% \usepackage{algorithm}
% \usepackage[noend]{algpseudocode}
% \usepackage{graphicx}
% \usepackage{bbm}

% \newcommand\alex[1]{\textcolor{red}{Aless: #1}}
% \newcommand\remi[1]{\textcolor{green}{Remi: #1}}
% \newcommand\yado[1]{\textcolor{orange}{M: #1}}

% \title{Debiasing by Example Forgetting}

% \author{Santa Klaus and His Dogs}

% \newcommand{\fix}{\marginpar{FIX}}
% \newcommand{\new}{\marginpar{NEW}}
% \newcommand\blfootnote[1]{%
%   \begingroup
%   \renewcommand\thefootnote{}\footnote{#1}%
%   \addtocounter{footnote}{-1}%
%   \endgroup
% }

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
% \begin{document}

% \maketitle

% \begin{abstract}
% \end{abstract}

% \section{Introduction}
% \input{intro}

% \begin{itemize}
%     \item what's the problem - copy from my mila - msr proposal here
%     \item what's example forgetting - present the previous paper. say that we show that most forgotten examples correspond to atypical examples, those that may not be solved by simple input-output correlations maybe, that's what motivate us in our investigation
%     \item state our goal here which is two-fold: 1) analyze whether forgetting events occur in mnli / squad ; 2) whether training only on forgotten could lead to more robust models , those relying less on spurious , more robust statistics .
%     \item say that in the example forgetting paper , we didn't test generalization in the out of domain setting but only from the same distribution.
%     \item hint at results and future work
% \end{itemize}

% \section{Methodology}

% \section{Experiments}

% \begin{itemize}
%     \item forgetting statistics on MNLI for BOW, BiLSTM and BERT
%     \item statistics of forgetting / unforgettable examples ; are they different ?
%     \item training only on forgotten examples don't help
%     \item finetune on forgotten examples help though
% \end{itemize}

% \begin{table}[h]
%     \centering
%     \begin{tabular}{l | l l}
%         Model & \#forgettables\\
%         \hline
%         BoW & & \\
%         BiLSTM & & \\
%         BERT & & \\
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}


% \begin{table}[h]
%     \centering
%     \begin{tabular}{l | l l}
%         Train source & HANS & MNLI \\
%         \hline
%         All & 60.0 & \\
%         BERT forgettables & & \\
%         BiLSTM forgettables                       & 54.0 & \\
%         BiLSTM forgettables + 20\% unforgettables & 57.5 & \\
%         BiLSTM forgettables + 30\% unforgettables & 57.5 & \\
%         BiLSTM forgettables + 50\% unforgettables & 57.5 & \\
%         BiLSTM forgettables + 80\% unforgettables & 57.5 & \\
%         \hline 
%         pretrian all, fine-tune on BERT forgettables & 67.8 & \\
%         pretrian all, fine-tune on BiLSTM forgettables & 69.2 & \\
%         pretrian all, fine-tune on BiLSTM 20\% unforgettables & 61.7 & \\
        
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}

% \section{Discussion and Conclusion}

% \bibliography{bibliography}
% \bibliographystyle{iclr2019_conference}



% \newpage
% \include{supp}

% \end{document}
