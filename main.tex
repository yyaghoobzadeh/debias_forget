\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{latexsym}
\usepackage{subcaption}
\usepackage{url}
\usepackage{stmaryrd}
\usepackage{color}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{multirow}
\usepackage{amssymb}
% \aclfinalcopy

% Paper-specific macros
\input{macros.tex}

\title{Robust Natural Language Inference Models \\ with Example Forgetting}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

% \author{%
%   Yadollah Yaghoobzadeh, Remi Tachet, T.J. Hazen, Alessandro Sordoni \\
%   Microsoft Research, Montr\'eal \\
%   \small \texttt{\{yayaghoo,retachet,tj.hazen,alsordon\}@microsoft.com}
  % examples of more authors
% }

\begin{document}

\maketitle

\begin{abstract}
We investigate whether example forgetting, a recently introduced measure of hardness of examples, can be used to select training examples in order to increase robustness of natural language understanding models to distribution shifts in a natural language inference task (MNLI) and a paraphrase task (QQP). We analyze forgetting events for those two datasets and provide evidence that forgettable examples under simpler models can be used to increase robustness of the recently proposed BERT and XLNET models. More precisely, we test MNLI and QQP trained models on HANS and PAWS, two datasets with a challenging distributional shift compared to the original ones, and observe a significant improvement in their performance with our method. Moreover, we show that the \emph{large} versions of BERT and XLNET are more robust than their \emph{base} counterparts, indicating that increasing the number of parameters not only improves generalization but also robustness. Finally, applying our technique to the large models also results in an improved robustness.
\end{abstract}

\section{Introduction}
\input{intro}

\section{Methodology}
\input{method}

\section{Results}
\label{sec:eval}
\subsection{MNLI and HANS}
\input{results_hans.tex}

\subsection{QQP and PAWS}
\label{sec:paws}
\input{paws.tex}


\iffalse
\section{Analysis}
\input{analysis}
\fi

\section{Discussion and Conclusion}
\input{discussion}

\section{Related Works}
\input{related.tex}

\clearpage
\bibliography{bibliography}
\bibliographystyle{acl_natbib}

\appendix
% \section{Detailed results on HANS}
\label{sec:detailedresults}
\input{hans_detailed_results}
\input{paws_detailed_results}

\end{document}

%TODO:
%- https://dawn.cs.stanford.edu/2019/03/22/glue/ (Signal 4: Dataset Slicing) 

% https://microsoft-my.sharepoint.com/:x:/r/personal/yayaghoo_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7B41975454-abff-488b-8f55-c550ad840d9f%7D&action=edit&activeCell=%27compare_baseModels%27!D9&wdInitialSession=c219a303-a524-465a-bdd6-28ac21d1312c&wdRldC=1

% \begin{table}[h]
%     \centering
%     \begin{tabular}{l | l l}
%         Train source & HANS & MNLI \\
%         \hline
%         All & 60.0 & \\
%         BERT forgettables & & \\
%         BiLSTM forgettables                       & 54.0 & \\
%         BiLSTM forgettables + 20\% unforgettables & 57.5 & \\
%         BiLSTM forgettables + 30\% unforgettables & 57.5 & \\
%         BiLSTM forgettables + 50\% unforgettables & 57.5 & \\
%         BiLSTM forgettables + 80\% unforgettables & 57.5 & \\
%         \hline 
%         pretrian all, fine-tune on BERT forgettables & 67.8 & \\
%         pretrian all, fine-tune on BiLSTM forgettables & 69.2 & \\
%         pretrian all, fine-tune on BiLSTM 20\% unforgettables & 61.7 & \\
        
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}

