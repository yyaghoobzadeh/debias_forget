
Neural network models have become ubiquitous in natural language processing applications, pushing the state-of-the-art in a large variety of tasks involving natural language understanding (NLU) and generation \cite{wu2016google,wang2019superglue}.
In the past year, significant improvements have been obtained by  training increasingly larger neural network language models on huge amounts of data openly available on the web and then fine-tuning those base models for each downstream task \cite{devlin2018bert,peters2018deep,liu2019multi}.

In spite of their impressive performance, empirical evidence suggests that these models are far from forming human-like representations of natural language. In fact, their predictions have been shown to be brittle on examples that slightly deviate from the training distribution but are still syntactically and semantically valid \cite{jia2017adversarial,linzen2019right}. In the context of natural language inference, evidence exists that they may not be robust when tested on examples obtained by applying simple meaning-preserving transformations such as passivization \cite{dasgupta2018evaluating}.
%As pointed out by~\cite{mitchell2018extrapolation}, NLP models should ``embody the symmetries that allow the same meaning to be expressed within multiple grammatical structures''.
%Another hypothesis is that current models do not learn~\emph{compositionality} rules \cite{montague1970universal},~i.e. how to compose smaller chunks into larger units of meaning, which has been linked to their poor systematic generalization capabilities \cite{loula2018rearranging,lake2017generalization,baan2019realization,hupkes2018learning}.
Increasing evidence supports the hypothesis that these models mainly tend to capture task- and dataset-specific biases such as shallow lexical word overlap features \cite{poliak2018hypothesis,dasgupta2018evaluating,linzen2019right,clark2019dont,zhang-etal-2019-paws}, which seems to be at odds with the common belief that they form high-level semantic representations of the input data \cite{bengio2009learning}. The reliance on highly predictive but brittle features is not confined to NLU tasks, it is also a perceived shortcoming of image classification models \cite{brendel2019approximating,geirhos2018imagenet,jacobsen2018excessive}. A relevant recent attempt at achieving robust learning when multiple ``views'' of the same training data are available can be found in \newcite{arjovsky2019invariant}.

Our general goal is to investigate whether it is possible to train NLU models that are more robust to distribution shifts. In particular, we investigate the possibility to identify a set of ``hard'' or ``atypical'' examples, which would unlikely be explained by simple heuristics and, if identified correctly and up-weighted during training, could enable learning more robust features. In the past, dataset re-sampling and weighting techniques have been studied in order to solve class imbalance problem~\cite{chawla2002smote} or covariate shift~\cite{sugiyama2007covariate}, notably by importance weighted empirical risk minimization. However, it has also been shown that up-weighting hard examples may be dangerous in the presence of outliers or noise \cite{chapelle2007training,Kumar10,toneva2018empirical}.

Concurrently to our work, \newcite{clark2019dont,he2019unlearn} and \newcite{mahabadi2019simple} give evidence towards the effectiveness of reweighting examples in building more robust NLU models. The authors generally assume~\emph{a priori} knowledge of the heuristics in the dataset and specifically weight examples that cannot be explained by those heuristics. In this work, we explore whether examples considered hard by ``weak'' or ``simple'' models (e.g. parametric models with a small number of parameters) naturally exclude the dataset heuristics without any prior knowledge of them. The underlying assumption is that weak models can more easily capture simple explanations of the training data but tend to underfit more complex patterns.

We consider~\emph{example forgetting} \cite{toneva2018empirical} as a model-dependent measure of ``hardness'' of an example. For a given task (\textit{e.g.} image classification), example forgetting is defined as the number of times the neural network shifts from properly classifying an example to making a mistake on the same example at the next training epoch. Examples with at least one forgetting event, the \emph{forgettable} examples, are rather atypical compared to the \emph{unforgettable} ones that contain very common features, prototypical of the class (\textit{e.g.} an occluded gray plane versus a white plane centered on a bright blue sky). It is interesting to note that forgetting events capture the dynamics of example learning, and not solely their loss at the end of training, as considered in \newcite{clark2019dont}. In this paper, we investigate whether up-weighting forgettable (aka hard) examples can help in training more robust models.  

\noindent
Our contributions are the following:
\begin{itemize}
    \item We first extend the results of \newcite{toneva2018empirical} by computing forgetting events in two popular NLP datasets,
    namely MNLI \cite{williams2017broad}, a natural language inference dataset, and QQP \cite{qqp}, a paraphrase dataset, for various architectures of increasing capacity. Our results show that weaker models tend to overfit on known heuristics for those datasets, a fact which contrasts with the observations by \newcite{toneva2018empirical} in the image setting, and open paths for future investigations.
    \item We propose a new training method to increase the robustness of NLP models, consisting in fine-tuning models on the forgettable examples of weaker models. Our approach does not assume \emph{a priori} knoweldge
    about the existing heuristics in the datasets. We apply our method to BERT and XLNET models and demonstrate a significant gain in performance on HANS and PAWS, two recent test sets designed to assess the robustness of language models: our best models achieve better performance than BERT, XLNET and recently proposed robust models.
    \item Finally, we show that the large versions of BERT and XLNET outperform their base counterparts on HANS and PAWS, demonstrating that larger models not only improve generalization but also robustness. Those large models also benefit from our method. For instance, \xlnetlarge goes from 76.1\% to 86.08\% on HANS. To the best of our knowledge, these are the first evaluations on HANS of \bertlarge and XLNET.
\end{itemize}

% We first extend the results of \newcite{toneva2018empirical} by computing forgetting events in MNLI \cite{williams2017broad}, a natural language inference dataset, and QQP \cite{qqp}, a paraphrase dataset, for various architectures of increasing capacity. The robustness of our models is verified by considering their performance on the recently proposed HANS and PAWS test sets. HANS \cite{linzen2019right} contains linguistically correct inference problems that cannot be solved by simple common heuristics, such as lexical overlap, usually learnt by models trained on the MNLI training set. Similarly, PAWS \cite{zhang-etal-2019-paws} contains paraphrase and non-paraphrase pairs with high lexical overlap, and is extremely challenging for models trained on the QQP training set. Our best models achieve better performance on the HANS and PAWS datasets than BERT, XLNET and recently proposed robust models. We also uncover interesting insights on forgettable examples for these datasets, which contrast with the observations by \newcite{toneva2018empirical} in the image setting, and open paths for future investigations.

% We first extend the results of \newcite{toneva2018empirical} by computing forgetting events in MNLI \cite{williams2017broad}, a natural language inference dataset, and QQP \cite{qqp}, a paraphrase dataset, for various architectures of increasing capacity. The robustness of our models is verified by considering their performance on the recently proposed HANS and PAWS test sets. HANS \cite{linzen2019right} contains linguistically correct inference problems that cannot be solved by simple common heuristics, such as lexical overlap, usually learnt by models trained on the MNLI training set. Similarly, PAWS \cite{zhang-etal-2019-paws} contains paraphrase and non-paraphrase pairs with high lexical overlap, and is extremely challenging for models trained on the QQP training set. Our best models achieve better performance on the HANS and PAWS datasets than BERT, XLNET and recently proposed robust models. We also uncover interesting insights on forgettable examples for these datasets, which contrast with the observations by \newcite{toneva2018empirical} in the image setting, and open paths for future investigations.

% Each such occurrence is coined a \emph{forgetting event}.
% The authors show that on various image datasets, the distribution of forgetting events is rather striking: numerous examples are never forgotten -- that is, once properly classified they remain so for the rest of training, while others withstand many a forgetting event.
% Visually inspecting those \emph{forgettable examples} shows they are rather atypical compared to the \emph{unforgettable} ones that are fairly prototypical of a class (\textit{e.g.} an occluded gray plane versus a white plane centered on a bright blue sky).
 


