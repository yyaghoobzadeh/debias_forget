
\subsection{Main result}

\begin{table*}[ht]
\caption{Results of BERT model trained on different sources of training examples. 
% We denote with $\Delta$ the absolute improvement with respect to the standard setting of fine-tuning on all the MNLI examples (first row). 
Lines from 2 to 7 correspond to finetuning only on subsets of MNLI data. The third block of results (lines from 8 to 11) corresponds to first finetuning BERT on all the MNLI data and then performing an additional stage of finetuning on selected examples. We also compare the performance to the recent baselines of~\cite{clark2019dont} (lines 12 to 14) and \cite{mahabadi2019simple} (line 15). They obtain slightly higher results for their base model~\textrm{All} but our best model outperforms their best result.}
\small
\label{tab:twoclass}
\centering
\begin{tabular}{llcccc}
\toprule
& Train examples & HANS & MNLI & HANS+MNLI  \\
\midrule
\small{1} & All & 58.3 & 84.5 & 64.8        \\
\midrule
\small{2} & BERT forgettables $_{\balancedbert}$   & 48.8                     & 38.9                         & 46.4\\
\small{3} & \hspace{0.1cm} Random $_{\balancedbert}$ & 51.9                   & 75.7                         & 57.8\\
\small{4} & BiLSTM forgettables $_{\balancedlstm}$ & 54.0                     & 66.8                         & 57.2 \\
\small{5} & \hspace{0.1cm} Random $_{\balancedlstm}$ & 51.1                   & 79.0                         & 58.0\\
\small{6} & BoW forgettables $_{\balancedbow}$    & 54.1                     & 68.3                         & 57.6 \\
\small{7} & \hspace{0.1cm} Random $_{\balancedbow}$ & 53.9                   & 79.6                         & 60.2\\
\midrule
&\emph{Additional stage of finetuning} \\
\small{8} & All + finetuning on BERT forgettables   & 70.8                     & 81.8                         & 73.5  \\
\small{9} & All + finetuning on BiLSTM forgettables &  \underline{74.0}                     & 82.5             & \underline{76.1} \\
\small{10} & \hspace{0.1cm} All + finetuning on Random $_{\balancedlstm}$       & 60.9                     & 84.4                         & 66.7  \\
\small{11} & All + finetuning on BoW forgettables    & \underline{73.7}                     & 82.4             & \underline{75.8} \\
\midrule
&\emph{From~\cite{clark2019dont}} & & & & \\
\small{12} & All & 62.4 & 84.2 & 67.8 \\
\small{13} & All (reweight) & 69.2 & 83.5 & 72.7 \\
\small{14} & Learned Mixin & 64.0 & 84.3 & 69.0\\
\small{15} & Product of Experts  & 66.5 & 84.0 & 70.8     \\
\bottomrule
\end{tabular}
\end{table*}

\iffalse
\begin{table*}
\caption{Stress test results}
\small
\label{tab:stress}
\centering
\begin{tabular}{ll llll | llll}
& Train examples & \multicolumn{4}{c}{Negation} & \multicolumn{4}{c}{Overlap}\\
& & E & C & N & Acc & E & C & N & ACC\\
\hline
\small{1} & All & 4.2 & 76.7 & 53.9 & 52.3 
& 17.0 & 77.0 & 55.6 & 55.3\\

\midrule
&\emph{Additional stage of finetuning} \\
\small{2} & All + finetuning on BiLSTM forgettables 
& 15.9 & 76.4 & 54.7 & 54.1 
& 23.6 & 76.8 & 55.6 & 56.2\\
\hline [0.76047002 0.09225789 0.53914917]
% &\emph{From~\cite{x}} & & & & \\
% \small{10} & All & 2.4 & 81.1 & 56.5 & -
% & 19.2 & 83.3 & 59.4 \\
% \small{11} & DRiFt-HYPO & 7.3 & 80.7 & 55.6 & - & 27.5 & 81.1 & 59.1 \\
% \small{12} & DRiFt-CBOW & 17.9 & 81.7 & 55.5 & - & 18.3 & 80.0 & 56.6 \\
% \small{13} & DRiFt-HAND & 4.3 & 80.6 & 55.5 & - & 15.0 & 81.9 & 57.4

\end{tabular}
\end{table*}
\fi

Our main results are presented in Table~\ref{tab:twoclass}. Lines 2 to 5 report results of fine-tuning BERT on different subsets of the MNLI dataset. This setting aligns with the setting presented in~\cite{toneva2018empirical} where the authors show that, in multiple image classification tasks, the same generalization performance can be obtained by training a model initialized randomly on its own forgettable examples. Our results suggest that this behavior may be task and/or architecture dependent: in our setting, training only forgettable examples particularly affects generalization performance on MNLI. The most extreme drop in performance is observed when BERT is only fine-tuned on its own forgettable examples (line 2) achieving an accuracy of 38.9\%. Training on BiLSTM (line 4) or BoW forgettables (line 6) examples causes a lesser drop in accuracy on MNLI although still noticeable. One of the possible reasons of the dramatic performance loss observed in line 2 is that BERT forgettables are significantly less than the counterparts from weaker baselines. 
In order to rule out this hypothesis, we train on a random subset of examples of the same size (\balancedbert, line 3). These results suggests that there is an intrinsic difficulty in BERT forgettables that deserves to be investigated in the future.
This is also the case for BiLSTM and BoW forgettables (lines 4 and 6) to some extend comparing to random samples with same size (lines 5 and 7).

In the second block of results (lines 8-11) we adopt a different strategy: we first fine-tune BERT on all the MNLI examples in order to get a reasonable prior for the task. We perform then an additional stage of fine-tuning on the subset of forgettable examples from each of the considered models. The results confirm that slightly biasing the model towards hard examples improves robustness at a slight cost in MNLI accuracy. 
Our best model is obtained by using the BiLSTM forgettable examples (line 9) achieving an accuracy of 74.0 on HANS, 
which constitute a +15.7 absolute improvement with respect to the base model in line 1 and +4.8 and +7.5 with respect to the concurrent models of \newcite{clark2019dont} and \newcite{mahabadi2019simple}. 
Results on line 10 confirm that the forgettable subsets of examples identified by BiLSTM is responsible for the improvement. 
Fine-tuning on BoW forgettables (line 11) is also comparable to BiLSTM forgettables (line 9). This shows that the choice of the weak model 
does not seem to matter and therefore does not need prior knowledge about the dataset biases.
An additional interesting observation is that BERT forgettables provide less improvement in robustness than BiLSTM or BoW. 
This seems to align with our hypothesis that weaker baselines capture more simple explanations in the dataset.




