\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{latexsym}
\usepackage{subcaption}
\usepackage{url}
\usepackage{stmaryrd}
\usepackage{color}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{multirow}
\usepackage{amssymb}
% \aclfinalcopy

% Paper-specific macros
\input{macros.tex}

\title{Up-weighting Forgettable Examples \\ for Down-weighting
Syntactic Inference Heuristics}



% \author{%
%   Yadollah Yaghoobzadeh, Remi Tachet, T.J. Hazen, Alessandro Sordoni \\
%   Microsoft Research, Montr\'eal \\
%   \small \texttt{\{yayaghoo,retachet,tj.hazen,alsordon\}@microsoft.com}
  % examples of more authors
% }

\begin{document}

\maketitle

\begin{abstract}
It is known now that there are heuristics 
models rely on when learning inference rules
from textual data. 
We propose a simple approach to increase robustness of strong natural language inference models against some frequent heuristics in the datasets.
We find a set of hard training examples, either by the target model itself or by a more simple one, and upweight them. 
To upweight, after training on the full dataset, we  fine-tune on the hard set only.
For selecting hard sets, we propose using forgettable
examples, i.e., examples that are learned and then forgotten
during training or never learned. 
This method is simple and effective while does not rely on prior knowledge of
the heuristics. 
On challenging evaluation datasets of HANS and PAWS designed to test syntactic heuristics learned by
models, we observe significant improvements in out-of-distribution generalization learning from
MNLI and QQP  datasets.
In addition, we make several novel findings about the performance in this setting, e.g.,
models with more parameters 
are more robust; longer training
results in more robustness; there is large correlation
between hardness and premise-hypothesis word overlap; it is important to include the hard examples at the beginning of the training.


\end{abstract}

\section{Introduction}

\input{intro}

\section{Robustness against common syntactic heuristics}
\input{method}

\section{Experimental setup}
\label{seupt}
\input{exp_setup}


\section{Results}
\label{sec:eval}
\subsection{MNLI and HANS}
\input{results_hans.tex}

\subsection{QQP and PAWS}
\label{sec:paws}
\input{paws.tex}

\subsection{FEVER}
\label{sec:fever}
\input{fever.tex}

\section{Analysis}
\input{analysis}

\section{Related Works}
\input{related.tex}

\section{Discussion and Conclusion}
\input{discussion}

\bibliography{bibliography}
\bibliographystyle{acl_natbib}

\appendix
\label{sec:appendix}
\input{hans_detailed_results}
\input{paws_detailed_results}
\input{appendix}
\end{document}

%TODO:
%- https://dawn.cs.stanford.edu/2019/03/22/glue/ (Signal 4: Dataset Slicing) 

% https://microsoft-my.sharepoint.com/:x:/r/personal/yayaghoo_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7B41975454-abff-488b-8f55-c550ad840d9f%7D&action=edit&activeCell=%27compare_baseModels%27!D9&wdInitialSession=c219a303-a524-465a-bdd6-28ac21d1312c&wdRldC=1

% \begin{table}[h]
%     \centering
%     \begin{tabular}{l | l l}
%         Train source & HANS & MNLI \\
%         \hline
%         All & 60.0 & \\
%         BERT forgettables & & \\
%         BiLSTM forgettables                       & 54.0 & \\
%         BiLSTM forgettables + 20\% unforgettables & 57.5 & \\
%         BiLSTM forgettables + 30\% unforgettables & 57.5 & \\
%         BiLSTM forgettables + 50\% unforgettables & 57.5 & \\
%         BiLSTM forgettables + 80\% unforgettables & 57.5 & \\
%         \hline 
%         pretrian all, fine-tune on BERT forgettables & 67.8 & \\
%         pretrian all, fine-tune on BiLSTM forgettables & 69.2 & \\
%         pretrian all, fine-tune on BiLSTM 20\% unforgettables & 61.7 & \\
        
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}

