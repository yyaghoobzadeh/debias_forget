
\paragraph{Curriculum Learning and Sample Weighting}
Curriculum learning is a paradigm that favors learning along a curriculum of examples of increasing difficulty~\citep{bengio2009curriculum}. This general idea has found success in a variety of areas since its introduction~\citep{Kumar10,lee2011learning,schaul2015prioritized}.~\citet{Kumar10} implemented their curriculum by considering easy the examples with a small loss. In our experiments, we empirically validate that unforgettable examples can be safely removed without compromising generalization.~\citet{Zhao2015,conf/icml/KatharopoulosF18} relate sample importance to the norm of its loss gradient with respect to the parameters of the network.~\citet{Fan2017,screenerNet,jiang18mentor} learn a curriculum directly from data in order to minimize the task loss.~\citet{jiang18mentor} also study the robustness of their method in the context of noisy examples. This relates to a rich literature on outlier detection and removal of examples with noisy labels~\citep{john1995robust,brodley1999identifying,sukhbaatar2014training,jiang18mentor}. We will provide evidence that noisy examples rank higher in terms of number of forgetting events. \cite{conf/icml/KohL17} borrow influence functions from robust statistics to evaluate the impact of the training examples on a model's predictions.

\paragraph{Deep Generalization} The study of the generalization properties of deep neural networks when trained by stochastic gradient descent has been the focus of several recent publications~\citep{zhang2016understanding, keskar2016large, Chaudhari2016,Advani2017HighdimensionalDO}. These studies suggest that the generalization error does not depend solely on the complexity of the hypothesis space. For instance, it has been demonstrated that over-parameterized models with many more parameters than training points can still achieve low test error~\citep{huang2017densely,Wang2018} while being complex enough to fit a dataset with completely random labels~\citep{zhang2016understanding}.
A possible explanation for this phenomenon is a form of implicit regularization performed by stochastic gradient descent: deep neural networks trained with SGD have been recently shown to converge to the maximum margin solution in the linearly separable case~\citep{Soudry2017,separable2}. In our work, we provide empirical evidence that generalization can be maintained when removing a substantial portion of the training examples and without restricting the complexity of the hypothesis class. This goes along the support vector interpretation provided by \citet{Soudry2017}.

% \paragraph{Continual Learning} \alex{If Maryia get good results, dump some stuff on continual learning, we'd need also to change the intro to make it a bit more concrete!}.

% \begin{itemize}
%    \item https://arxiv.org/pdf/1801.00904.pdf @\textbf{Remi}, here they apply the stuff they do to reinforcement learning in place of the PER (prioritized experience replay). I think it will be worth thinking about it.
%    \begin{itemize}
%        \item 2016: https://arxiv.org/pdf/1606.04232.pdf hard-removal of examples in the training set in the context of noisy labels. Model obtains low accuracy on MNIST, even lower than removing random subsets of the training set. Their model obtains $~93\%$ test accuracy using $3000$ training samples, whereas we have found that retaining $3000$ random examples for training results in a mean accuracy of $98.02\%$ across $3$ seeds with standard deviation of $0.07$. In comparison, our approach of retaining the most forgotten $3000$ examples results in $98.9\%$ accuracy.
%        \item 1999 https://arxiv.org/pdf/1106.0219.pdf identification of noisy labels , nice discussion about the difference between "noise" (to filter) and "exceptions" (to retain)
% \end{itemize}
% \end{itemize}