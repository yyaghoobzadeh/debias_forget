@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}


@article{belkin2018reconciling,
  abstract = {Breakthroughs in machine learning are rapidly changing science and society,
yet our fundamental understanding of this technology has lagged far behind.
Indeed, one of the central tenets of the field, the bias-variance trade-off,
appears to be at odds with the observed behavior of methods used in the modern
machine learning practice. The bias-variance trade-off implies that a model
should balance under-fitting and over-fitting: rich enough to express
underlying structure in data, simple enough to avoid fitting spurious patterns.
However, in the modern practice, very rich models such as neural networks are
trained to exactly fit (i.e., interpolate) the data. Classically, such models
would be considered over-fit, and yet they often obtain high accuracy on test
data. This apparent contradiction has raised questions about the mathematical
foundations of machine learning and their relevance to practitioners.
  In this paper, we reconcile the classical understanding and the modern
practice within a unified performance curve. This "double descent" curve
subsumes the textbook U-shaped bias-variance trade-off curve by showing how
increasing model capacity beyond the point of interpolation results in improved
performance. We provide evidence for the existence and ubiquity of double
descent for a wide spectrum of models and datasets, and we posit a mechanism
for its emergence. This connection between the performance and the structure of
machine learning models delineates the limits of classical analyses, and has
implications for both the theory and practice of machine learning.},
  added-at = {2019-09-22T03:21:45.000+0200},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  biburl = {https://www.bibsonomy.org/bibtex/2e0dfdb35c453ca6cf7a8676f1f513ab7/kirk86},
  description = {[1812.11118] Reconciling modern machine learning practice and the bias-variance trade-off},
  interhash = {5eece341319b51b6f79c5bd2f38c6679},
  intrahash = {e0dfdb35c453ca6cf7a8676f1f513ab7},
  keywords = {deep-learning machine-learning},
  note = {cite arxiv:1812.11118},
  timestamp = {2019-09-22T03:21:45.000+0200},
  title = {Reconciling modern machine learning practice and the bias-variance
  trade-off},
  url = {http://arxiv.org/abs/1812.11118},
  year = 2018
}


@article{sugiyama2007covariate,
  title={Covariate shift adaptation by importance weighted cross validation},
  author={Sugiyama, Masashi and Krauledat, Matthias and M{\~A}{\v{z}}ller, Klaus-Robert},
  journal={Journal of Machine Learning Research},
  volume={8},
  number={May},
  pages={985--1005},
  year={2007}
}

@article{chawla2002smote,
  title={SMOTE: synthetic minority over-sampling technique},
  author={Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  journal={Journal of artificial intelligence research},
  volume={16},
  pages={321--357},
  year={2002}
}

@inproceedings{conf/icml/KohL17,
  added-at = {2017-07-25T00:00:00.000+0200},
  author = {Koh, Pang Wei and Liang, Percy},
  biburl = {https://www.bibsonomy.org/bibtex/22e7e5af1d322e4386e41b63a6144dbf3/dblp},
  booktitle = {ICML},
  editor = {Precup, Doina and Teh, Yee Whye},
  ee = {http://proceedings.mlr.press/v70/koh17a.html},
  interhash = {756776881aa53a744646c6ff48db12d5},
  intrahash = {2e7e5af1d322e4386e41b63a6144dbf3},
  keywords = {dblp},
  pages = {1885-1894},
  publisher = {JMLR.org},
  series = {JMLR Workshop and Conference Proceedings},
  timestamp = {2017-07-26T11:38:39.000+0200},
  title = {Understanding Black-box Predictions via Influence Functions.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2017.html#KohL17},
  volume = 70,
  year = 2017
}



@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009},
  organization={ACM}
}

@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@inproceedings{lee2011learning,
  title={Learning the easy things first: Self-paced visual category discovery},
  author={Lee, Yong Jae and Grauman, Kristen},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on},
  pages={1721--1728},
  year={2011},
  organization={IEEE}
}

@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}

@inproceedings{Kumar10,
author = {Kumar, M Pawan and Packer, Benjamin and Koller, Daphne},
pages = {1--9},
title = {{Self-Paced Learning for Latent Variable Models}},
booktitle={Proc. of NIPS},
year={2010}
}


@article{intrinsic,
  added-at = {2018-08-13T00:00:00.000+0200},
  author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  biburl = {https://www.bibsonomy.org/bibtex/29d7b5ee33f6f6b82649142854e0493f1/dblp},
  ee = {http://arxiv.org/abs/1804.08838},
  interhash = {741dc3cc06e1876c504b8547e8c9d77a},
  intrahash = {9d7b5ee33f6f6b82649142854e0493f1},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T15:18:19.000+0200},
  title = {Measuring the Intrinsic Dimension of Objective Landscapes.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1804.html#abs-1804-08838},
  volume = {abs/1804.08838},
  year = 2018
}

@misc{qqp,
    title = {Quora Question Pairs.},
    author = {Kaggle},
    url={https://www.kaggle.com/c/quora-question-pairs/overview},
    year = 2017
}

@misc{kingma2014method,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization
of stochastic objective functions, based on adaptive estimates of lower-order
moments. The method is straightforward to implement, is computationally
efficient, has little memory requirements, is invariant to diagonal rescaling
of the gradients, and is well suited for problems that are large in terms of
data and/or parameters. The method is also appropriate for non-stationary
objectives and problems with very noisy and/or sparse gradients. The
hyper-parameters have intuitive interpretations and typically require little
tuning. Some connections to related algorithms, on which Adam was inspired, are
discussed. We also analyze the theoretical convergence properties of the
algorithm and provide a regret bound on the convergence rate that is comparable
to the best known results under the online convex optimization framework.
Empirical results demonstrate that Adam works well in practice and compares
favorably to other stochastic optimization methods. Finally, we discuss AdaMax,
a variant of Adam based on the infinity norm.},
  added-at = {2016-05-25T11:58:12.000+0200},
  author = {Kingma, Diederik and Ba, Jimmy},
  biburl = {https://www.bibsonomy.org/bibtex/205cc8a7efbed1ccfe69aa97384177c5f/galeone},
  description = {[1412.6980] Adam: A Method for Stochastic Optimization},
  interhash = {57d2ac873f398f21bb94790081e80394},
  intrahash = {05cc8a7efbed1ccfe69aa97384177c5f},
  keywords = {adam optimization sgd},
  note = {cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference  for Learning Representations, San Diego, 2015},
  timestamp = {2016-05-25T11:58:12.000+0200},
  title = {Adam: A Method for Stochastic Optimization},
  url = {http://arxiv.org/abs/1412.6980},
  year = 2014
}


@misc{zagoruyko2016residual,
  abstract = {Deep residual networks were shown to be able to scale up to thousands of
layers and still have improving performance. However, each fraction of a
percent of improved accuracy costs nearly doubling the number of layers, and so
training very deep residual networks has a problem of diminishing feature
reuse, which makes these networks very slow to train. To tackle these problems,
in this paper we conduct a detailed experimental study on the architecture of
ResNet blocks, based on which we propose a novel architecture where we decrease
depth and increase width of residual networks. We call the resulting network
structures wide residual networks (WRNs) and show that these are far superior
over their commonly used thin and very deep counterparts. For example, we
demonstrate that even a simple 16-layer-deep wide residual network outperforms
in accuracy and efficiency all previous deep residual networks, including
thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR,
SVHN, COCO, and significant improvements on ImageNet. Our code and models are
available at https://github.com/szagoruyko/wide-residual-networks},
  added-at = {2018-04-02T12:42:40.000+0200},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  biburl = {https://www.bibsonomy.org/bibtex/2e7014a5e5e5037b6f1092874bdc96a6a/achakraborty},
  description = {[1605.07146] Wide Residual Networks},
  interhash = {784559792e88c8ce111af09592315f9c},
  intrahash = {e7014a5e5e5037b6f1092874bdc96a6a},
  keywords = {2016 computer-vision deep-learning},
  note = {cite arxiv:1605.07146},
  timestamp = {2018-04-02T12:42:40.000+0200},
  title = {Wide Residual Networks},
  url = {http://arxiv.org/abs/1605.07146},
  year = 2016
}

@misc{yang2019xlnet,
  abstract = {With the capability of modeling bidirectional contexts, denoising
autoencoding based pretraining like BERT achieves better performance than
pretraining approaches based on autoregressive language modeling. However,
relying on corrupting the input with masks, BERT neglects dependency between
the masked positions and suffers from a pretrain-finetune discrepancy. In light
of these pros and cons, we propose XLNet, a generalized autoregressive
pretraining method that (1) enables learning bidirectional contexts by
maximizing the expected likelihood over all permutations of the factorization
order and (2) overcomes the limitations of BERT thanks to its autoregressive
formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the
state-of-the-art autoregressive model, into pretraining. Empirically, XLNet
outperforms BERT on 20 tasks, often by a large margin, and achieves
state-of-the-art results on 18 tasks including question answering, natural
language inference, sentiment analysis, and document ranking.},
  added-at = {2019-07-03T19:07:32.000+0200},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  biburl = {https://www.bibsonomy.org/bibtex/2b758258da935db4bc1a57b5f6c9d94c6/deepforce},
  description = {[1906.08237] XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  interhash = {cd85caa3241071a53ea5c86eadae8de8},
  intrahash = {b758258da935db4bc1a57b5f6c9d94c6},
  keywords = {language_modeling nlp tpu transfer_learning},
  note = {cite arxiv:1906.08237Comment: Pretrained models and code are available at  https://github.com/zihangdai/xlnet},
  timestamp = {2019-07-03T19:07:32.000+0200},
  title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  url = {http://arxiv.org/abs/1906.08237},
  year = 2019
}




@article{deepdouble,
  title={Deep double descent: where bigger models and more data hurt},
  author={Nakkiran, P. and Kaplun, G and Bansal, Y and Yang, T and Barak, B and Sutskever, I},
  year={2019},
  url={https://mltheory.org/deep.pdf}
}


@article{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  journal={In Proc. of ICML},
  year={2017}
}

@article{ravi2016optimization,
  title={Optimization as a model for few-shot learning},
  author={Ravi, Sachin and Larochelle, Hugo},
  journal={In Proc. of ICLR},
  year={2017}
}


@article{Fan2017,
author = {Fan, Yang and Tian, Fei and Qin, Tao and Bian, Jiang},
title = {{Learning What Data to Learn}},
arxivId = {arXiv:1702.08635v1},
year={2017}
}


@inproceedings{chang2017active,
author = {Chang, Haw-Shiuan and Learned-Miller, Erik and McCallum, Andrew},
booktitle = {Advances in Neural Information Processing Systems},
pages = {1002--1012},
title = {{Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples}},
year = {2017}
}

@article{separable2,
  added-at = {2018-08-13T00:00:00.000+0200},
  author = {Xu, Tengyu and Zhou, Yi and Ji, Kaiyi and Liang, Yingbin},
  biburl = {https://www.bibsonomy.org/bibtex/28389d62297eeac9505e7a7bba6000b4f/dblp},
  ee = {http://arxiv.org/abs/1806.04339},
  interhash = {0f49502cecc4519ee9bcb79a610b9475},
  intrahash = {8389d62297eeac9505e7a7bba6000b4f},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T15:21:52.000+0200},
  title = {Convergence of SGD in Learning ReLU Models with Separable Data.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1806.html#abs-1806-04339},
  volume = {abs/1806.04339},
  year = 2018
}


@inproceedings{Zhao2015,
author = {Zhao, Peilin and Zhang, Tong},
title = {{Stochastic Optimization with Importance Sampling for Regularized Loss Minimization}},
booktitle={Proc. of ICML},
year = {2015}
}


@InProceedings{jiang18mentor,
  title = 	 {{M}entor{N}et: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels},
  author = 	 {Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  year = 	 {2018},
  publisher = 	 {PMLR},
  abstract = 	 {Recent deep networks are capable of memorizing the entire data even when the labels are completely random. To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called MentorNet, to supervise the training of the base deep networks, namely, StudentNet. During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct. Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data. Notably, to the best of our knowledge, we achieve the best-published result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels.}
}


@inproceedings{conf/icml/KatharopoulosF18,
  added-at = {2018-07-13T00:00:00.000+0200},
  author = {Katharopoulos, Angelos and Fleuret, François},
  biburl = {https://www.bibsonomy.org/bibtex/23b3e77feb0b4bfc095c96183e5a47cad/dblp},
  booktitle = {ICML},
  editor = {Dy, Jennifer G. and Krause, Andreas},
  ee = {http://proceedings.mlr.press/v80/katharopoulos18a.html},
  interhash = {b1d431ba9e4f1ce76b8068b382d5e58f},
  intrahash = {3b3e77feb0b4bfc095c96183e5a47cad},
  keywords = {dblp},
  pages = {2530-2539},
  publisher = {JMLR.org},
  series = {JMLR Workshop and Conference Proceedings},
  timestamp = {2018-07-18T11:44:31.000+0200},
  title = {Not All Samples Are Created Equal: Deep Learning with Importance Sampling.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2018.html#KatharopoulosF18},
  volume = 80,
  year = 2018
}




@inproceedings{john1995robust,
  title={Robust decision trees: removing outliers from databases},
  author={John, George H},
  booktitle={Proceedings of the First International Conference on Knowledge Discovery and Data Mining},
  pages={174--179},
  year={1995},
  organization={AAAI Press}
}

@article{sukhbaatar2014training,
author = {Sukhbaatar, Sainbayar and Bruna, Joan and Paluri, Manohar and Bourdev, Lubomir and Fergus, Rob},
journal = {arXiv preprint arXiv:1406.2080},
title = {{Training convolutional networks with noisy labels}},
year = {2014}
}

@article{brodley1999identifying,
  title={Identifying mislabeled training data},
  author={Brodley, Carla E and Friedl, Mark A},
  journal={Journal of artificial intelligence research},
  volume={11},
  pages={131--167},
  year={1999}
}

@article{goodfellow2014empirical,
  title={An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks},
  author={Goodfellow, Ian J and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
  journal={stat},
  volume={1050},
  pages={6},
  year={2014}
}


@article{cichon2015branch,
  title={Branch-specific dendritic Ca2+ spikes cause persistent synaptic plasticity},
  author={Cichon, Joseph and Gan, Wen-Biao},
  journal={Nature},
  volume={520},
  number={7546},
  pages={180--185},
  year={2015},
  publisher={Nature Research}
}


@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier}
}


@article{kirkpatrick2017overcoming,
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Others},
journal = {Proceedings of the national academy of sciences},
pages = {201611835},
publisher = {National Acad Sciences},
title = {{Overcoming catastrophic forgetting in neural networks}},
year = {2017}
}

@article{french1999catastrophic,
  title={Catastrophic forgetting in connectionist networks},
  author={French, Robert M},
  journal={Trends in cognitive sciences},
  volume={3},
  number={4},
  pages={128--135},
  year={1999},
  publisher={Elsevier}
}

@inproceedings{lopez2017gradient,
  title={Gradient episodic memory for continual learning},
  author={Lopez-Paz, David and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6467--6476},
  year={2017}
}

@article{Ritter2018,
abstract = {We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework, where we recursively approximate the posterior after every task with a Gaussian, leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode, which is typically intractable for modern architectures. In order to make our method scalable, we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90{\%} test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset, substantially outperforming related methods for overcoming catastrophic forgetting.},
archivePrefix = {arXiv},
arxivId = {1805.07810},
author = {Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
eprint = {1805.07810},
file = {:home/alsordon/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ritter, Botev, Barber - 2018 - Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting.pdf:pdf},
mendeley-groups = {NIPS/NIPS2018},
title = {{Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting}},
url = {http://arxiv.org/abs/1805.07810},
year = {2018}
}


@article{Soudry2017,
author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
eprint = {1710.10345},
file = {:Users/Sordoni/Library/Application Support/Mendeley Desktop/Downloaded/9ee749d7f1059de4742c71338276f50a4ceeada7.pdf:pdf},
title = {{The Implicit Bias of Gradient Descent on Separable Data}},
url = {http://arxiv.org/abs/1710.10345},
year = {2017}
}

@article{implicit_bias,
  added-at = {2018-08-13T00:00:00.000+0200},
  author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  biburl = {https://www.bibsonomy.org/bibtex/2f8e445b3a446acde0a1031d8232b94e0/dblp},
  ee = {http://arxiv.org/abs/1412.6614},
  interhash = {e302965c2d22ac1a6924d47fa8b61114},
  intrahash = {f8e445b3a446acde0a1031d8232b94e0},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T14:21:10.000+0200},
  title = {In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1412.html#NeyshaburTS14},
  volume = {abs/1412.6614},
  year = 2014
}


@inproceedings{huang2017densely,
author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {4700--4708},
title = {{Densely Connected Convolutional Networks}},
year = {2017}
}

@article{Wang2018,
author = {Wang, Huan and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
doi = {arXiv:1809.07402v1},
eprint = {1809.07402},
file = {:Users/Sordoni/Library/Application Support/Mendeley Desktop/Downloaded/2169acce9014fd4ce462da494b71a3d2ef1c8191.pdf:pdf},
keywords = {generalization,hessian,pac-bayes,perturbation},
pages = {1--23},
title = {{Identifying Generalization Properties in Neural Networks}},
url = {http://arxiv.org/abs/1809.07402},
year = {2018}
}

@article{journals/nn/WangYMC17,
  added-at = {2017-12-28T00:00:00.000+0100},
  author = {Wang, Linnan and Yang, Yi and Min, Renqiang and Chakradhar, Srimat T.},
  biburl = {https://www.bibsonomy.org/bibtex/25e5bbe3efe9268206b6da210c4210c36/dblp},
  ee = {https://doi.org/10.1016/j.neunet.2017.06.003},
  interhash = {32086b7dfe5b8fcf85dc04f272fe71b2},
  intrahash = {5e5bbe3efe9268206b6da210c4210c36},
  journal = {Neural Networks},
  keywords = {dblp},
  pages = {219-229},
  timestamp = {2017-12-29T11:54:35.000+0100},
  title = {Accelerating deep neural network training with inconsistent stochastic gradient descent.},
  url = {http://dblp.uni-trier.de/db/journals/nn/nn93.html#WangYMC17},
  volume = 93,
  year = 2017
}




@article{Chaudhari2016,
author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
isbn = {978-3-642-04273-7},
journal = {ICLR '17},
title = {{Entropy-SGD: Biasing Gradient Descent Into Wide Valleys}},
year = {2016}
}


@article{screenerNet,
  added-at = {2018-08-13T00:00:00.000+0200},
  author = {Kim, Tae-Hoon and Choi, Jonghyun},
  biburl = {https://www.bibsonomy.org/bibtex/2557eaa11becb7e74825829fc555bcbf8/dblp},
  ee = {http://arxiv.org/abs/1801.00904},
  interhash = {3edda93a8bbcada51fb5f6507bde500b},
  intrahash = {557eaa11becb7e74825829fc555bcbf8},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T13:37:05.000+0200},
  title = {ScreenerNet: Learning Curriculum for Neural Networks.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1801.html#abs-1801-00904},
  volume = {abs/1801.00904},
  year = 2018
}




@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{dynamics,
  author = {R. Tachet and M. Pezeshki and S. Shabanian and A. Courville and Y. Bengio},
  doi = {arXiv:1809.06848v1},
  title = {On the Learning Dynamics of Deep Neural Networks},
  url = {https://arxiv.org/abs/1809.06848},
  year = 2018
}


@article{flatminima,
  added-at = {2008-03-11T14:52:34.000+0100},
  author = {Hochreiter, S. and Schmidhuber, J.},
  biburl = {https://www.bibsonomy.org/bibtex/2b826d12133183831f3632da166a69c0d/idsia},
  citeulike-article-id = {2381167},
  interhash = {bd8101710e3afdad5fd19dd82bd3d83e},
  intrahash = {b826d12133183831f3632da166a69c0d},
  journal = {Neural Computation},
  keywords = {juergen},
  number = 1,
  pages = {1--42},
  priority = {2},
  timestamp = {2008-03-11T14:54:49.000+0100},
  title = {Flat minima},
  volume = 9,
  year = 1997
}

@article{kleinbergSGD,
  added-at = {2018-03-01T00:00:00.000+0100},
  author = {Kleinberg, Robert and Li, Yuanzhi and Yuan, Yang},
  biburl = {https://www.bibsonomy.org/bibtex/2e8c0c24275afaac7edf492b557a96753/dblp},
  ee = {http://arxiv.org/abs/1802.06175},
  interhash = {254ffa1eced5f99ec283e65402f03691},
  intrahash = {e8c0c24275afaac7edf492b557a96753},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-03-02T11:35:21.000+0100},
  title = {An Alternative View: When Does SGD Escape Local Minima?},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1802.html#abs-1802-06175},
  volume = {abs/1802.06175},
  year = 2018
}

@article{Advani2017HighdimensionalDO,
  title={High-dimensional dynamics of generalization error in neural networks},
  author={Madhu S. Advani and Andrew M. Saxe},
  journal={CoRR},
  year={2017},
  volume={abs/1710.03667}
}

@article{function_map,
  added-at = {2018-08-13T00:00:00.000+0200},
  author = {Perez, Guillermo Valle and Camargo, Chico Q. and Louis, Ard A.},
  biburl = {https://www.bibsonomy.org/bibtex/28c1a237c765490e1bd29d7dc377a936a/dblp},
  ee = {http://arxiv.org/abs/1805.08522},
  interhash = {357854aa455eadec4f5a450aaecb792a},
  intrahash = {8c1a237c765490e1bd29d7dc377a936a},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T15:15:21.000+0200},
  title = {Deep learning generalizes because the parameter-function map is biased towards simple functions.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1805.html#abs-1805-08522},
  volume = {abs/1805.08522},
  year = 2018
}

@article{cifar,
  author = {Alex Krizhevsky},
  title = {Learning Multiple Layers of Features from Tiny Images},
  url = {https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf},
  year = 2009
}

@article{mnist,
  author = {Y. LeCun and C. Cortes C. and C. Burges},
  title = {THE MNIST DATABASE of handwritten digits.},
  url = {http://yann.lecun.com/exdb/mnist/},
  year = 1999
}


@inproceedings{poliak2018hypothesis,
  title={Hypothesis Only Baselines in Natural Language Inference},
  author={Poliak, Adam and Naradowsky, Jason and Haldar, Aparajita and Rudinger, Rachel and Van Durme, Benjamin},
  booktitle={Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics},
  pages={180--191},
  year={2018}
}

@article{hjelm2018learning,
  title={Learning deep representations by mutual information estimation and maximization},
  author={Hjelm, R Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Trischler, Adam and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1808.06670},
  year={2018}
}

@inproceedings{tishby2015deep,
  title={Deep learning and the information bottleneck principle},
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={2015 IEEE Information Theory Workshop (ITW)},
  pages={1--5},
  year={2015},
  organization={IEEE}
}

@article{ON,
  author    = {Yikang Shen and
               Shawn Tan and
               Alessandro Sordoni and
               Aaron C. Courville},
  title     = {Ordered Neurons: Integrating Tree Structures into Recurrent Neural
               Networks},
  journal   = {CoRR},
  volume    = {abs/1810.09536},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.09536},
  archivePrefix = {arXiv},
  eprint    = {1810.09536},
}

@article{cubuk2018autoaugment,
  title={Autoaugment: Learning augmentation policies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  journal={arXiv preprint arXiv:1805.09501},
  year={2018}
}

@article{berthelot2019mixmatch,
  title={MixMatch: A Holistic Approach to Semi-Supervised Learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin},
  journal={arXiv preprint arXiv:1905.02249},
  year={2019}
}

@article{sennrich2015improving,
  title={Improving neural machine translation models with monolingual data},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1511.06709},
  year={2015}
}

@article{conneau2018you,
  title={What you can cram into a single vector: Probing sentence embeddings for linguistic properties},
  author={Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Lo{\"\i}c and Baroni, Marco},
  journal={arXiv preprint arXiv:1805.01070},
  year={2018}
}

@article{hill2019learning,
  title={Learning to Make Analogies by Contrasting Abstract Relational Structure},
  author={Hill, Felix and Santoro, Adam and Barrett, David GT and Morcos, Ari S and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1902.00120},
  year={2019}
}

@article{andreas2019measuring,
  title={Measuring Compositionality in Representation Learning},
  author={Andreas, Jacob},
  journal={arXiv preprint arXiv:1902.07181},
  year={2019}
}

@article{loula2018rearranging,
  title={Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks},
  author={Loula, Joao and Baroni, Marco and Lake, Brenden M},
  journal={arXiv preprint arXiv:1807.07545},
  year={2018}
}

@article{arjovsky2019invariant,
  title={Invariant Risk Minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}

@article{chapelle2007training,
  title={Training a support vector machine in the primal},
  author={Chapelle, Olivier},
  journal={Neural computation},
  volume={19},
  number={5},
  pages={1155--1178},
  year={2007},
  publisher={MIT Press}
}

@article{montague1970universal,
  title={Universal grammar},
  author={Montague, Richard},
  journal={Theoria},
  volume={36},
  number={3},
  pages={373--398},
  year={1970},
  publisher={Wiley Online Library}
}

@article{marcus2018deep,
  title={Deep learning: A critical appraisal},
  author={Marcus, Gary},
  journal={arXiv preprint arXiv:1801.00631},
  year={2018}
}

@inproceedings{andreas2016neural,
  title={Neural module networks},
  author={Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={39--48},
  year={2016}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}


@article{ettinger2018assessing,
  title={Assessing composition in sentence vector representations},
  author={Ettinger, Allyson and Elgohary, Ahmed and Phillips, Colin and Resnik, Philip},
  journal={arXiv preprint arXiv:1809.03992},
  year={2018}
}

@article{toneva2018empirical,
  title={An empirical study of example forgetting during deep neural network learning},
  author={Toneva, Mariya and Sordoni, Alessandro and Combes, Remi Tachet des and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J},
  journal={arXiv preprint arXiv:1812.05159},
  year={2018}
}

@article{hupkes2018learning,
  title={Learning compositionally through attentive guidance},
  author={Hupkes, Dieuwke and Singh, Anand and Korrel, Kris and Kruszewski, German and Bruni, Elia},
  journal={arXiv preprint arXiv:1805.09657},
  year={2018}
}

@article{baan2019realization,
  title={On the Realization of Compositionality in Neural Networks},
  author={Baan, Joris and Leible, Jana and Nikolaus, Mitja and Rau, David and Ulmer, Dennis and Baumg{\"a}rtner, Tim and Hupkes, Dieuwke and Bruni, Elia},
  journal={arXiv preprint arXiv:1906.01634},
  year={2019}
}

@misc{clark2019dont,
title={Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases},
author={Christopher Clark and Mark Yatskar and Luke Zettlemoyer},
year={2019},
eprint={1909.03683},
archivePrefix={arXiv},
primaryClass={cs.CL}
}

@article{gururangan2018annotation,
  title={Annotation artifacts in natural language inference data},
  author={Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel R and Smith, Noah A},
  journal={arXiv preprint arXiv:1803.02324},
  year={2018}
}

@article{linzen2019right,
  title={Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference},
  author={R. Thomas McCoy and
               Ellie Pavlick and
               Tal Linzen},
  journal={Proceedings of the ACL},
  year={2019}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}



@article{Emami2018TheHC,
  title={The Hard-CoRe Coreference Corpus: Removing Gender and Number Cues for Difficult Pronominal Anaphora Resolution},
  author={Ali Emami and Paul Trichelair and Adam Trischler and Kaheer Suleman and Hannes Schulz and Jackie Chi Kit Cheung},
  journal={CoRR},
  year={2018},
  volume={abs/1811.01747}
}

@article{clark2018semi,
  title={Semi-supervised sequence modeling with cross-view training},
  author={Clark, Kevin and Luong, Minh-Thang and Manning, Christopher D and Le, Quoc V},
  journal={arXiv preprint arXiv:1809.08370},
  year={2018}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1126--1135},
  year={2017},
  organization={JMLR. org}
}

@article{dasgupta2018evaluating,
  title={Evaluating compositionality in sentence embeddings},
  author={Dasgupta, Ishita and Guo, Demi and Stuhlm{\"u}ller, Andreas and Gershman, Samuel J and Goodman, Noah D},
  journal={arXiv preprint arXiv:1802.04302},
  year={2018}
}

@article{brendel2019approximating,
  title={Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet},
  author={Brendel, Wieland and Bethge, Matthias},
  journal={arXiv preprint arXiv:1904.00760},
  year={2019}
}

@article{drozdov2019unsupervised,
  title={Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders},
  author={Drozdov, Andrew and Verga, Pat and Yadav, Mohit and Iyyer, Mohit and McCallum, Andrew},
  journal={arXiv preprint arXiv:1904.02142},
  year={2019}
}

@article{bengio2019meta,
  title={A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms},
  author={Bengio, Yoshua and Deleu, Tristan and Rahaman, Nasim and Ke, Rosemary and Lachapelle, S{\'e}bastien and Bilaniuk, Olexa and Goyal, Anirudh and Pal, Christopher},
  journal={arXiv preprint arXiv:1901.10912},
  year={2019}
}

@article{URNNG,
  author    = {Yoon Kim and
               Alexander M. Rush and
               Lei Yu and
               Adhiguna Kuncoro and
               Chris Dyer and
               G{\'{a}}bor Melis},
  title     = {Unsupervised Recurrent Neural Network Grammars},
  journal   = {CoRR},
  year      = {2019}
}

@article{geirhos2018imagenet,
  title={ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness},
  author={Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A and Brendel, Wieland},
  journal={arXiv preprint arXiv:1811.12231},
  year={2018}
}

@article{bahdanau2018systematic,
  title={Systematic Generalization: What Is Required and Can It Be Learned?},
  author={Bahdanau, Dzmitry and Murty, Shikhar and Noukhovitch, Michael and Nguyen, Thien Huu and de Vries, Harm and Courville, Aaron},
  journal={arXiv preprint arXiv:1811.12889},
  year={2018}
}

@article{baker2018deep,
  title={Deep convolutional networks do not classify based on global object shape},
  author={Baker, Nicholas and Lu, Hongjing and Erlikhman, Gennady and Kellman, Philip J},
  journal={PLoS computational biology},
  volume={14},
  number={12},
  pages={e1006613},
  year={2018},
  publisher={Public Library of Science}
}



@article{lake2017generalization,
  title={Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  author={Lake, Brenden M and Baroni, Marco},
  journal={arXiv preprint arXiv:1711.00350},
  year={2017}
}

@article{mitchell2018extrapolation,
  title={Extrapolation in NLP},
  author={Mitchell, Jeff and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
  journal={arXiv preprint arXiv:1805.06648},
  year={2018}
}

@article{jacobsen2018excessive,
  title={Excessive Invariance Causes Adversarial Vulnerability},
  author={Jacobsen, J{\"o}rn-Henrik and Behrmann, Jens and Zemel, Richard and Bethge, Matthias},
  journal={arXiv preprint arXiv:1811.00401},
  year={2018}
}

@article{shen2017neural,
  title={Neural Language Modeling by Jointly Learning Syntax and Lexicon},
  author={Shen, Yikang and Lin, Zhouhan and Huang, Chin-Wei and Courville, Aaron},
  journal={arXiv preprint arXiv:1711.02013},
  year={2017}
}

@article{dehaene2015neural, title={The neural representation of sequences: from transition probabilities to algebraic patterns and linguistic trees}, author={Dehaene, Stanislas and Meyniel, Florent and Wacongne, Catherine and Wang, Liping and Pallier, Christophe}, journal={Neuron}, volume={88}, number={1}, pages={2--19}, year={2015}, publisher={Elsevier} }

@misc{koopman2013introduction,
  title={An introduction to syntactic analysis and theory},
  author={Koopman, Hilda and Sportiche, Dominique and Stabler, Edward},
  year={2013},
  publisher={Wiley-Blackwell}
}

@article{chomsky1956three,
  title={Three models for the description of language},
  author={Chomsky, Noam},
  journal={IRE Transactions on information theory},
  volume={2},
  number={3},
  pages={113--124},
  year={1956},
  publisher={IEEE}
}

@book{chomsky1965,
  abstract = {Standard-TG},
  added-at = {2007-02-16T16:53:53.000+0100},
  address = {Cambridge},
  author = {Chomsky, Noam},
  biburl = {https://www.bibsonomy.org/bibtex/2634ef0838fe6ad268105e0be37a3be8c/vittorio.loreto},
  citeulike-article-id = {867124},
  interhash = {10dff7148609d28159a0602fcd6c7cbb},
  intrahash = {634ef0838fe6ad268105e0be37a3be8c},
  keywords = {RMP_CFL generative-grammar linguistics syntax chomsky 1965},
  priority = {1},
  publisher = {The MIT Press},
  timestamp = {2007-02-16T16:53:53.000+0100},
  title = {Aspects of the Theory of Syntax},
  url = {http://www.amazon.com/Aspects-Theory-Syntax-Noam-Chomsky/dp/0262530074},
  year = 1965
}

@inproceedings{lakretz2019emergence,
  title={The emergence of number and syntax units in LSTM language models},
  author={Lakretz, Yair and Kruszewski, German and Desbordes, Theo and Hupkes, Dieuwke and Dehaene, Stanislas and Baroni, Marco},
  booktitle={Proc. of NAACL},
  year={2019}
}

@inproceedings{gulordava2018colorless,
  title={Colorless Green Recurrent Networks Dream Hierarchically},
  author={Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
  booktitle={Proc. of NAACL},
  pages={1195--1205},
  year={2018}
}

@inproceedings{dyer2016recurrent,
  title={Recurrent Neural Network Grammars},
  author={Dyer, Chris and Kuncoro, Adhiguna and Ballesteros, Miguel and Smith, Noah A},
  booktitle={Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={199--209},
  year={2016}
}

@article{linzen2016assessing,
  title={Assessing the ability of LSTMs to learn syntax-sensitive dependencies},
  author={Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  journal={arXiv preprint arXiv:1611.01368},
  year={2016}
}

@article{bowman2016fast,
  title={A fast unified model for parsing and sentence understanding},
  author={Bowman, Samuel R and Gauthier, Jon and Rastogi, Abhinav and Gupta, Raghav and Manning, Christopher D and Potts, Christopher},
  journal={arXiv preprint arXiv:1603.06021},
  year={2016}
}


@book{aho1972theory,
  title={The theory of parsing, translation, and compiling},
  author={Aho, Alfred V and Ullman, Jeffrey D},
  volume={1},
  year={1972},
  publisher={Prentice-Hall Englewood Cliffs, NJ}
}


@inproceedings{nivre2003efficient,
  title={An efficient algorithm for projective dependency parsing},
  author={Nivre, Joakim},
  booktitle={Proceedings of the 8th International Workshop on Parsing Technologies (IWPT},
  year={2003},
  organization={Citeseer}
}

@inproceedings{shieber1983sentence,
  title={Sentence disambiguation by a shift-reduce parsing technique},
  author={Shieber, Stuart M},
  booktitle={Proceedings of the 21st annual meeting on Association for Computational Linguistics},
  pages={113--118},
  year={1983},
  organization={Association for Computational Linguistics}
}

@article{dowty2007compositionality,
  title={4},
  author={Dowty, David},
  journal={Direct compositionality},
  volume={14},
  pages={23--101},
  year={2007},
  publisher={Citeseer}
}

@article{merityRegOpt,
  title={{Regularizing and Optimizing LSTM Language Models}},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1708.02182},
  year={2017}
}


@article{melis2017state,
  title={On the state of the art of evaluation in neural language models},
  author={Melis, G{\'a}bor and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1707.05589},
  year={2017}
}


@article{bengio2003neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal={Journal of machine learning research},
  volume={3},
  number={Feb},
  pages={1137--1155},
  year={2003}
}


@article{karpathy2015visualizing,
  title={Visualizing and understanding recurrent networks},
  author={Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
  journal={arXiv preprint arXiv:1506.02078},
  year={2015}
}

@article{yang2017breaking,
  title={Breaking the softmax bottleneck: A high-rank RNN language model},
  author={Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W},
  journal={arXiv preprint arXiv:1711.03953},
  year={2017}
}
@book{sandra2014morphological,
  title={Morphological Structure, Lexical Representation and Lexical Access (RLE Linguistics C: Applied Linguistics): A Special Issue of Language and Cognitive Processes},
  author={Sandra, Dominiek and Taft, Marcus},
  year={2014},
  publisher={Routledge}
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@article{tai2015improved,
  title={Improved semantic representations from tree-structured long short-term memory networks},
  author={Tai, Kai Sheng and Socher, Richard and Manning, Christopher D},
  journal={arXiv preprint arXiv:1503.00075},
  year={2015}
}

@inproceedings{socher2010learning,
  title={Learning continuous phrase representations and syntactic parsing with recursive neural networks},
  author={Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
  booktitle={Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop},
  volume={2010},
  pages={1--9},
  year={2010}
}

@article{zhang2015top,
  title={Top-down tree long short-term memory networks},
  author={Zhang, Xingxing and Lu, Liang and Lapata, Mirella},
  journal={arXiv preprint arXiv:1511.00060},
  year={2015}
}


@article{alvarez2016tree,
  title={Tree-structured decoding with doubly-recurrent neural networks},
  author={Alvarez-Melis, David and Jaakkola, Tommi S},
  year={2016}
}

@article{zhou2017generative,
  title={Generative Neural Machine for Tree Structures},
  author={Zhou, Ganbin and Luo, Ping and Cao, Rongyu and Xiao, Yijun and Lin, Fen and Chen, Bo and He, Qing},
  journal={CoRR},
  year={2017}
}



@article{marvin2018targeted,
  title={Targeted Syntactic Evaluation of Language Models},
  author={Marvin, Rebecca and Linzen, Tal},
  journal={arXiv preprint arXiv:1808.09031},
  year={2018}
}

@article{williams2017learning,
  title={Learning to parse from a semantic objective: It works. Is it syntax?},
  author={Williams, Adina and Drozdov, Andrew and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1709.01121},
  year={2017}
}

@inproceedings{schulman2015gradient,
  title={Gradient estimation using stochastic computation graphs},
  author={Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3528--3536},
  year={2015}
}

@article{bowman2015tree,
  title={Tree-structured composition in neural networks without tree-structured architectures},
  author={Bowman, Samuel R and Manning, Christopher D and Potts, Christopher},
  journal={arXiv preprint arXiv:1506.04834},
  year={2015}
}

@article{bowman2014recursive,
  title={Recursive neural networks can learn logical semantics},
  author={Bowman, Samuel R and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1406.1827},
  year={2014}
}

@article{jain2019attention,
  title={Attention is not Explanation},
  author={Jain, Sarthak and Wallace, Byron C},
  journal={arXiv preprint arXiv:1902.10186},
  year={2019}
}

@article{liu2019multi,
  title={Multi-Task Deep Neural Networks for Natural Language Understanding},
  author={Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:1901.11504},
  year={2019}
}

@article{peters2018deep,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1802.05365},
  year={2018}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@inproceedings{jia2017adversarial,
  author = {R. Jia and P. Liang},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {Adversarial Examples for Evaluating Reading Comprehension Systems},
  year = {2017},
}


@article{brennan2019hierarchical,
  title={Hierarchical structure guides rapid linguistic predictions during naturalistic listening},
  author={Brennan, Jonathan R and Hale, John T},
  journal={PloS one},
  volume={14},
  number={1},
  pages={e0207741},
  year={2019},
  publisher={Public Library of Science}
}

@article{nangia2018listops,
  title={ListOps: A Diagnostic Dataset for Latent Tree Learning},
  author={Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.06028},
  year={2018}
}

@article{havrylov2019cooperative,
  title={Cooperative Learning of Disjoint Syntax and Semantics},
  author={Havrylov, Serhii and Kruszewski, Germ{\'a}n and Joulin, Armand},
  journal={arXiv preprint arXiv:1902.09393},
  year={2019}
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  journal={Proceedings of ACL},
  year={2019}
}

@article{wang2019superglue,
  title={SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1905.00537},
  year={2019}
}

@article{yogatama2019learning,
  title={Learning and evaluating general linguistic intelligence},
  author={Yogatama, Dani and d'Autume, Cyprien de Masson and Connor, Jerome and Kocisky, Tomas and Chrzanowski, Mike and Kong, Lingpeng and Lazaridou, Angeliki and Ling, Wang and Yu, Lei and Dyer, Chris and others},
  journal={arXiv preprint arXiv:1901.11373},
  year={2019}
}

@article {Fitz546325,
	author = {Fitz, Hartmut and Uhlmann, Marvin and van den Broek, Dick and Duarte, Renato and Hagoort, Peter and Petersson, Karl Magnus},
	title = {Neuronal memory for language processing},
	elocation-id = {546325},
	year = {2019},
	doi = {10.1101/546325},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {In language processing, an interpretation is computed incrementally within memory while utterances unfold in time. Here, we investigate the nature of this processing memory in a spiking network model of sentence comprehension. We show that the history dependence of neuronal responses endows circuits of biological neurons with adequate memory to assign semantic roles and resolve binding relations between words in a stream of language input. A neurobiological read-write memory is proposed where short-lived spiking activity encodes information into coupled dynamic variables that move at slower timescales. This state-dependent network does not rely on persistent activity, excitatory feedback, or synaptic plasticity for storage. Instead, information is maintained in adaptive neuronal conductances and can be accessed directly during comprehension without cued retrieval of previous input words. This work provides a step towards a computational neurobiology of language.},
	URL = {https://www.biorxiv.org/content/early/2019/02/11/546325},
	eprint = {https://www.biorxiv.org/content/early/2019/02/11/546325.full.pdf},
	journal = {bioRxiv}
}


@inproceedings{el1996hierarchical,
  title={Hierarchical recurrent neural networks for long-term dependencies},
  author={El Hihi, Salah and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={493--499},
  year={1996}
}

@article{schmidhuber1991neural,
  title={Neural sequence chunkers},
  author={Schmidhuber, J{\"u}rgen},
  year={1991},
  publisher={Citeseer}
}

@techreport{lin1998learning,
  title={Learning long-term dependencies is not as difficult with NARX recurrent neural networks},
  author={Lin, Tsungnan and Horne, Bill G and Tino, Peter and Giles, C Lee},
  year={1998}
}


@article{koutnik2014clockwork,
  title={A clockwork rnn},
  author={Koutnik, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, Juergen},
  journal={arXiv preprint arXiv:1402.3511},
  year={2014}
}

@article{marcus1993building,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitchell P and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
  journal={Computational linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993},
  publisher={MIT Press}
}

@article{mikolov2012statistical,
  title={Statistical language models based on neural networks},
  author={Mikolov, Tom{\'a}{\v{s}}},
  journal={Presentation at Google, Mountain View, 2nd April},
  year={2012}
}

@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model},
  author={Mikolov, Tom{\'a}{\v{s}} and Karafi{\'a}t, Martin and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Eleventh Annual Conference of the International Speech Communication Association},
  year={2010}
}

@article{htut2018grammar,
  title={Grammar Induction with Neural Language Models: An Unusual Replication},
  author={Htut, Phu Mon and Cho, Kyunghyun and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1808.10000},
  year={2018}
}

@article{zaremba2014recurrent,
  title={Recurrent neural network regularization},
  author={Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1409.2329},
  year={2014}
}

@inproceedings{gal2016dropout,
  title={Dropout as a Bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016}
}

@inproceedings{gal2016theoretically,
  title={A theoretically grounded application of dropout in recurrent neural networks},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={Advances in neural information processing systems},
  pages={1019--1027},
  year={2016}
}

@inproceedings{kim2016character,
  title={Character-Aware Neural Language Models.},
  author={Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M},
  booktitle={AAAI},
  pages={2741--2749},
  year={2016}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@article{grave2016improving,
  title={Improving neural language models with a continuous cache},
  author={Grave, Edouard and Joulin, Armand and Usunier, Nicolas},
  journal={arXiv preprint arXiv:1612.04426},
  year={2016}
}

@article{inan2016tying,
  title={Tying word vectors and word classifiers: A loss framework for language modeling},
  author={Inan, Hakan and Khosravi, Khashayar and Socher, Richard},
  journal={arXiv preprint arXiv:1611.01462},
  year={2016}
}

@article{zilly2016recurrent,
  title={Recurrent highway networks},
  author={Zilly, Julian Georg and Srivastava, Rupesh Kumar and Koutn{\'\i}k, Jan and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1607.03474},
  year={2016}
}

@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}

@inproceedings{klein2002generative,
  title={A generative constituent-context model for improved grammar induction},
  author={Klein, Dan and Manning, Christopher D},
  booktitle={Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
  pages={128--135},
  year={2002},
  organization={Association for Computational Linguistics}
}

@article{klein2005natural,
  title={Natural language grammar induction with a generative constituent-context model},
  author={Klein, Dan and Manning, Christopher D},
  journal={Pattern recognition},
  volume={38},
  number={9},
  pages={1407--1419},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{bod2006all,
  title={An all-subtrees approach to unsupervised parsing},
  author={Bod, Rens},
  booktitle={Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics},
  pages={865--872},
  year={2006},
  organization={Association for Computational Linguistics}
}

@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@article{chung2016hierarchical,
  title={Hierarchical multiscale recurrent neural networks},
  author={Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1609.01704},
  year={2016}
}

@article{chelba2000structured,
  title={Structured language modeling},
  author={Chelba, Ciprian and Jelinek, Frederick},
  journal={Computer Speech \& Language},
  volume={14},
  number={4},
  pages={283--332},
  year={2000},
  publisher={Elsevier}
}


@inproceedings{charniak2001immediate,
  title={Immediate-head parsing for language models},
  author={Charniak, Eugene},
  booktitle={Proceedings of the 39th Annual Meeting on Association for Computational Linguistics},
  pages={124--131},
  year={2001},
  organization={Association for Computational Linguistics}
}


@article{roark2001probabilistic,
  title={Probabilistic top-down parsing and language modeling},
  author={Roark, Brian},
  journal={Computational linguistics},
  volume={27},
  number={2},
  pages={249--276},
  year={2001},
  publisher={MIT Press}
}

@inproceedings{klein2003accurate,
  title={Accurate unlexicalized parsing},
  author={Klein, Dan and Manning, Christopher D},
  booktitle={Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1},
  pages={423--430},
  year={2003},
  organization={Association for Computational Linguistics}
}

@book{chomsky2002syntactic,
  title={Syntactic structures},
  author={Chomsky, Noam and Lightfoot, David W},
  year={2002},
  publisher={Walter de Gruyter}
}

@article{bengio2009learning,
  title={Learning deep architectures for AI},
  author={Bengio, Yoshua and others},
  journal={Foundations and trends{\textregistered} in Machine Learning},
  volume={2},
  number={1},
  pages={1--127},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Research}
}

@article{schmidhuber2015deep,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={61},
  pages={85--117},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{wu2017sequence,
  title={Sequence-to-dependency neural machine translation},
  author={Wu, Shuangzhi and Zhang, Dongdong and Yang, Nan and Li, Mu and Zhou, Ming},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  volume={1},
  pages={698--707},
  year={2017}
}

@inproceedings{kuncoro2018lstms,
  title={LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better},
  author={Kuncoro, Adhiguna and Dyer, Chris and Hale, John and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  volume={1},
  pages={1426--1436},
  year={2018}
}


@article{siegelmann1991turing,
  title={Turing computability with neural nets},
  author={Siegelmann, Hava T and Sontag, Eduardo D},
  journal={Applied Mathematics Letters},
  volume={4},
  number={6},
  pages={77--80},
  year={1991},
  publisher={Elsevier}
}

@article{gers2001lstm,
  title={LSTM recurrent networks learn simple context-free and context-sensitive languages},
  author={Gers, Felix A and Schmidhuber, E},
  journal={IEEE Transactions on Neural Networks},
  volume={12},
  number={6},
  pages={1333--1340},
  year={2001},
  publisher={IEEE}
}


@article{yogatama2018memory,
  title={Memory Architectures in Recurrent Neural Network Language Models},
  author={Yogatama, Dani and Miao, Yishu and Melis, Gabor and Ling, Wang and Kuncoro, Adhiguna and Dyer, Chris and Blunsom, Phil},
  year={2018}
}


@article{pollack1990recursive,
  title={Recursive distributed representations},
  author={Pollack, Jordan B},
  journal={Artificial Intelligence},
  volume={46},
  number={1-2},
  pages={77--105},
  year={1990},
  publisher={Elsevier}
}

@article{shi2018tree,
  title={On Tree-Based Neural Sentence Modeling},
  author={Shi, Haoyue and Zhou, Hao and Chen, Jiaze and Li, Lei},
  journal={arXiv preprint arXiv:1808.09644},
  year={2018}
}

@inproceedings{jacob2018learning,
  title={Learning Hierarchical Structures On-The-Fly with a Recurrent-Recursive Model for Sequences},
  author={Jacob, Athul Paul and Lin, Zhouhan and Sordoni, Alessandro and Bengio, Yoshua},
  booktitle={Proceedings of The Third Workshop on Representation Learning for NLP},
  pages={154--158},
  year={2018}
}

@article{williams2018latent,
  title={Do latent tree learning models identify meaningful structure in sentences?},
  author={Williams, Adina and Drozdov*, Andrew and Bowman, Samuel R},
  journal={Transactions of the Association of Computational Linguistics},
  volume={6},
  pages={253--267},
  year={2018},
  publisher={MIT Press}
}

@inproceedings{vinyals2015grammar,
  title={Grammar as a foreign language},
  author={Vinyals, Oriol and Kaiser, {\L}ukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2773--2781},
  year={2015}
}

@inproceedings{choi2018learning,
  title={Learning to compose task-specific tree structures},
  author={Choi, Jihun and Yoo, Kang Min and Lee, Sang-goo},
  booktitle={Proceedings of the 2018 Association for the Advancement of Artificial Intelligence (AAAI). and the 7th International Joint Conference on Natural Language Processing (ACL-IJCNLP)},
  year={2018}
}

@article{yogatama2016learning,
  title={Learning to compose words into sentences with reinforcement learning},
  author={Yogatama, Dani and Blunsom, Phil and Dyer, Chris and Grefenstette, Edward and Ling, Wang},
  journal={arXiv preprint arXiv:1611.09100},
  year={2016}
}

@inproceedings{press2017using,
  title={Using the Output Embedding to Improve Language Models},
  author={Press, Ofir and Wolf, Lior},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  volume={2},
  pages={157--163},
  year={2017}
}

@inproceedings{chen1995bayesian,
  title={Bayesian grammar induction for language modeling},
  author={Chen, Stanley F},
  booktitle={Proceedings of the 33rd annual meeting on Association for Computational Linguistics},
  pages={228--235},
  year={1995},
  organization={Association for Computational Linguistics}
}


@inproceedings{cohen2011unsupervised,
  title={Unsupervised structure prediction with non-parallel multilingual guidance},
  author={Cohen, Shay B and Das, Dipanjan and Smith, Noah A},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  pages={50--61},
  year={2011},
  organization={Association for Computational Linguistics}
}

@inproceedings{hsu2012identifiability,
  title={Identifiability and unmixing of latent parse trees},
  author={Hsu, Daniel J and Kakade, Sham M and Liang, Percy S},
  booktitle={Advances in neural information processing systems},
  pages={1511--1519},
  year={2012}
}

@inproceedings{naseem2011using,
  title={Using semantic cues to learn syntax},
  author={Naseem, Tahira and Barzilay, Regina},
  booktitle={Twenty-Fifth AAAI Conference on Artificial Intelligence},
  year={2011}
}

@inproceedings{das1992learning,
  title={Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory},
  author={Das, Sreerupa and Giles, C Lee and Sun, Guo-Zheng},
  booktitle={Proceedings of The Fourteenth Annual Conference of Cognitive Science Society. Indiana University},
  pages={14},
  year={1992}
}

@article{sun2017neural,
  title={The neural network pushdown automaton: Model, stack and learning simulations},
  author={Sun, Guo-Zheng and Giles, C Lee and Chen, Hsing-Hen and Lee, Yee-Chun},
  journal={arXiv preprint arXiv:1711.05738},
  year={2017}
}

@inproceedings{grefenstette2015learning,
  title={Learning to transduce with unbounded memory},
  author={Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1828--1836},
  year={2015}
}

@inproceedings{joulin2015inferring,
  title={Inferring algorithmic patterns with stack-augmented recurrent nets},
  author={Joulin, Armand and Mikolov, Tomas},
  booktitle={Advances in neural information processing systems},
  pages={190--198},
  year={2015}
}

@inproceedings{rippel2014learning,
  title={Learning ordered representations with nested dropout},
  author={Rippel, Oren and Gelbart, Michael and Adams, Ryan},
  booktitle={International Conference on Machine Learning},
  pages={1746--1754},
  year={2014}
}



@inproceedings{zhu2015long,
  title={Long short-term memory over recursive structures},
  author={Zhu, Xiaodan and Sobihani, Parinaz and Guo, Hongyu},
  booktitle={International Conference on Machine Learning},
  pages={1604--1612},
  year={2015}
}

@article{xu2016cached,
  title={Cached long short-term memory neural networks for document-level sentiment classification},
  author={Xu, Jiacheng and Chen, Danlu and Qiu, Xipeng and Huang, Xuangjing},
  journal={arXiv preprint arXiv:1610.04989},
  year={2016}
}


@misc{he2019unlearn,
    title={Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual},
    author={He He and Sheng Zha and Haohan Wang},
    year={2019},
    eprint={1908.10763},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{mahabadi2019simple,
    title={Simple but effective techniques to reduce biases},
    author={Rabeeh Karimi Mahabadi and James Henderson},
    year={2019},
    eprint={1909.06321},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{Nangia_2019,
   title={Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark},
   booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
   author={Nangia, Nikita and Bowman, Samuel R.},
   year={2019}
}

@article{dasgupta2018,
  author    = {Ishita Dasgupta and
               Demi Guo and
               Andreas Stuhlm{\"{u}}ller and
               Samuel J. Gershman and
               Noah D. Goodman},
  title     = {Evaluating Compositionality in Sentence Embeddings},
  journal   = {CoRR},
  volume    = {abs/1802.04302},
  year      = {2018}
}

@inproceedings{zhang-etal-2019-paws,
    title = "{PAWS}: Paraphrase Adversaries from Word Scrambling",
    author = "Zhang, Yuan  and
      Baldridge, Jason  and
      He, Luheng",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    pages = "1298--1308"
}

@article{roberta2019,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
}


@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}


@article{spanBERT2019,
  author    = {Mandar Joshi and
               Danqi Chen and
               Yinhan Liu and
               Daniel S. Weld and
               Luke Zettlemoyer and
               Omer Levy},
  title     = {SpanBERT: Improving Pre-training by Representing and Predicting Spans},
  journal   = {CoRR},
  volume    = {abs/1907.10529},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.10529},
  archivePrefix = {arXiv},
  eprint    = {1907.10529},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1907-10529},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chen-etal-2017-enhanced,
    title = "Enhanced {LSTM} for Natural Language Inference",
    author = "Chen, Qian  and
      Zhu, Xiaodan  and
      Ling, Zhen-Hua  and
      Wei, Si  and
      Jiang, Hui  and
      Inkpen, Diana",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1152",
    doi = "10.18653/v1/P17-1152",
    pages = "1657--1668",
    abstract = "Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6{\%} on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result{---}it further improves the performance even when added to the already very strong model.",
}

@inproceedings{parikh-etal-2016-decomposable,
    title = "A Decomposable Attention Model for Natural Language Inference",
    author = {Parikh, Ankur  and
      T{\"a}ckstr{\"o}m, Oscar  and
      Das, Dipanjan  and
      Uszkoreit, Jakob},
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1244",
    doi = "10.18653/v1/D16-1244",
    pages = "2249--2255",
}

@article{gong2017natural,
  title={Natural language inference over interaction space},
  author={Gong, Yichen and Luo, Heng and Zhang, Jian},
  journal={arXiv preprint arXiv:1709.04348},
  year={2017}
}

