% \newcommand\ent{$E$}
% \newcommand\neu{$N$}
% \newcommand\con{$C$}
% \newcommand{\nent}{$\neg E$}
% \newcommand{\bt}[2]{#2}

\begin{table*}[t]
\begin{tabular}{p{\textwidth}}
\toprule
\textbf{Source HANS example} \vspace{1mm} \\
\hspace{2mm} \emph{The banker thanked the tourist.} $\longarrownot\longrightarrow$ \emph{The tourist thanked the banker.} \\
\\
\textbf{Nearest neighbors by BERT} \vspace{1mm} \\
\hspace{2mm} \emph{The model simulates the size of the pool of exchangeable base cations in the soil.} \\ \hspace{2mm} $\longrightarrow$ \emph{The model simulates the size of the pool of base cations in the dirt.} \vspace{2mm} \\
\hspace{2mm} \emph{(Or click to read my summary of Wolfe's and Rose's positions.)} \\
\hspace{2mm} $\longrightarrow$ \emph{To read my summary of Wolfe's position, click.} \\
\\
\textbf{Nearest neighbors by our Robust BERT} \vspace{1mm} \\
\hspace{2mm} \emph{He sat patiently as she talked}. $\longarrownot\longrightarrow$
\emph{She sat patiently as he spoke} \vspace{2mm} \\
\hspace{2mm} \emph{And Gates and Appiah would have to be thanked for opening the door.} \\ \hspace{2mm} $\longarrownot\longrightarrow$
\emph{Gates and Appiah were thanked for opening the door.} \\
\bottomrule
\end{tabular}
\caption{Two nearest neighbors for one HANS non-entailment ($\longarrownot\longrightarrow$)  example show-casing how our robust model (line 10 in Table 2) pushes supporting $\longarrownot\longrightarrow$ training data closer compared to standard BERT (MNLI). 
To compute nearest neighbors from the BERT models, the embedding  of the special token (CLS) is assumed as the representation of an example and cosine is used as the similarity metric.}
\label{tab:NNs}
\end{table*}

\iffalse
\begin{table*}[]
\begin{tabular}{p{0.45\textwidth} p{0.45\textwidth} }
\\
\multicolumn{2}{l}{\emph{The HANS example}}
\\
\toprule
\multicolumn{2}{l}{
\begin{tabular}[c]{@{}l@{}}
\emph{The banker thanked the tourist.} $\longarrownot\longrightarrow$ \emph{The tourist thanked the banker.} \\
\end{tabular}
}
\\
\toprule
\multicolumn{1}{c}{\textbf{Nearest neighbors of BERT (MNLI)}}
& \multicolumn{1}{c}{\textbf{Nearest neighbors of our Robust BERT}}     \\
\midrule
\begin{tabular}[t]{@{}p{0.45\textwidth}@{}}
\emph{The model simulates the size of the pool of exchangeable base cations in the soil.} $\longrightarrow$ \emph{The model simulates the size of the pool of base cations in the dirt.} \\
% \\ \emph{Label}: \ent
\end{tabular} & 
\begin{tabular}[t]{@{}p{0.45\textwidth}@{}}
\emph{He sat patiently as she talked}. $\longarrownot\longrightarrow$
\emph{She sat patiently as he spoke}
\end{tabular}
\
\\
% \cmidrule(lr){2-2}
\midrule
\begin{tabular}[c]{@{}p{0.45\textwidth}@{}}
P:	(Or click to read my summary of Wolfe's and Rose's positions.)
H:	To read my summary of Wolfe's position, click.
label: \ent
\end{tabular}
&
\begin{tabular}[c]{@{}p{0.45\textwidth}@{}}
\emph{And Gates and Appiah would have to be thanked for opening the door.} $\longarrownot\longrightarrow$
\emph{Gates and Appiah were thanked for opening the door.} \\
\end{tabular}
\\
\end{tabular}
\caption{Two nearest neighbors for one HANS non-entailment (\nent{})  example show-casing how our robust model (line 10 in Table 2) pushes supporting \nent{} training data closer compared to standard BERT (MNLI). 
To compute nearest neighbors from the BERT models, the embedding 
of the special token (CLS) is assumed as the representation of
an example and cosine is used as the similarity metric.}
\label{tab:NNs}
\end{table*}
\fi


\begin{table*}[]
\small
\centering
\begin{tabular}{lccccccc}
    & \multicolumn{3}{c}{\textbf{Paraphrase}} & & \multicolumn{3}{c}{\textbf{Non-Paraphrase}} \\

                            Model    & \emph{All}    & \emph{High}   & \emph{Low}   & & \emph{All}     & \emph{High}    & \emph{Low}     \\
\toprule

BERT                       & \textbf{90.0} & \textbf{90.8} & \textbf{88.9} &  & 92.2 & 85.6 & 95.0 \\
BERT + \flstm & 85.2 & 84.9 & 85.8 &  & \textbf{93.0} & \textbf{87.3} & \textbf{95.4} \\   
BERT + \fbow    & 87.3 & 87.2 & 87.4 &  & 92.6 & 86.4 & 95.2 \\   
\bottomrule
\end{tabular}
\caption{Fine-grained accuracy results of BERT on QQP development set before and after finetuning on forgettables. 
We split the evaluation set into High ($>$ mean) and Low ($<$ mean) word-overlap examples,
where word-overlap is measures under Jaccard Index between two sentences. Similar observations than in the case of MNLI hold.
%COPIED FROM TABLE 3 fine_mnli: Finetuning hurts \pph{} results in both High and Low but improves \npph{} on High.
%This is in line with the fine-grained results of HANS depicted in Figure \ref{fig:fine_eval_baselines} where all examples have High word-overlap.
}
\label{fine_qqp}   
\end{table*}
