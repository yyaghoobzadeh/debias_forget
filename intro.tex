
Neural network models have become ubiquitous in natural language processing applications, pushing the state-of-the-art in a large variety of tasks involving natural language understanding (NLU) and generation \cite{wu2016google,wang2019superglue}.
In the past year, significant improvements have been obtained by  training increasingly larger neural network language models on huge amounts of data openly available on the web and then fine-tuning those base models for each downstream task \cite{devlin2018bert,peters2018deep,liu2019multi}.

In spite of their impressive performance, empirical evidence suggests that these models are far from forming human-like representations of natural language. In fact, their predictions have been shown to be brittle on examples that slightly deviate from the training distribution but are still syntactically and semantically valid \cite{jia2017adversarial,linzen2019right}. In the context of natural language inference, evidence exists that they may not be robust when tested on examples obtained by applying simple meaning-preserving transformations such as passivization \cite{dasgupta2018evaluating}.
%As pointed out by~\cite{mitchell2018extrapolation}, NLP models should ``embody the symmetries that allow the same meaning to be expressed within multiple grammatical structures''.
%Another hypothesis is that current models do not learn~\emph{compositionality} rules \cite{montague1970universal},~i.e. how to compose smaller chunks into larger units of meaning, which has been linked to their poor systematic generalization capabilities \cite{loula2018rearranging,lake2017generalization,baan2019realization,hupkes2018learning}.
Increasing evidence supports the hypothesis that these models mainly tend to capture task- and dataset-specific biases such as shallow lexical word overlap features \cite{poliak2018hypothesis,dasgupta2018evaluating,linzen2019right,clark2019dont}, which seems to be at odds with the common belief that they form high-level semantic representations of the input data \cite{bengio2009learning}. The reliance on highly predictive but brittle features is not confined to NLU tasks, it is also a perceived shortcoming of image classification models \cite{brendel2019approximating,geirhos2018imagenet,jacobsen2018excessive}. A relevant recent attempt at achieving robust learning when multiple ``views'' of the same training data are available can be found in \newcite{arjovsky2019invariant}.

Our general goal is to investigate whether it is possible to train more robust NLU models. In particular, we investigate the possibility to identify a set of ``hard'' or ``atypical'' examples, which would unlikely be explained by simple heuristics and, if identified correctly and up-weighted during training, could enable learning more robust features. In the past, dataset re-sampling and weighting techniques have been studied in order to solve class imbalance problem~\cite{chawla2002smote} or covariate shift~\cite{sugiyama2007covariate}, notably by importance weighted empirical risk minimization. However, it has also been shown that up-weighting hard examples may be dangerous in the presence of outliers or noise \cite{chapelle2007training,Kumar10,toneva2018empirical}.

Concurrently to our work, \newcite{clark2019dont} and \newcite{mahabadi2019simple} give evidence towards the effectiveness of reweighting examples in building more robust NLU models. The authors assume~\emph{a priori} knowledge of the heuristics in the dataset and specifically weight examples that cannot be explained by those heuristics. In this work, we explore whether examples considered hard by ``weak'' or ``simple'' models (e.g. parametric models with a small number of parameters) naturally exclude the dataset heuristics without any prior knowledge of them. The underlying assumption is that weak models can more easily capture simple explanations of the training data but underfit more complex patterns.

We consider~\emph{example forgetting} \cite{toneva2018empirical} as a model-dependent measure of ``hardness'' of an example. For a given task (\textit{e.g.} image classification), example forgetting is defined as the number of times the neural network shifts from properly classifying an example to making a mistake on the same example at the next training epoch. Examples with a large number of forgetting events, the \emph{forgettable} examples, are rather atypical compared to the \emph{unforgettable} ones that contain very common features, prototypical of the class (\textit{e.g.} an occluded gray plane versus a white plane centered on a bright blue sky). It is interesting to note that forgetting events capture the dynamics of example learning, and not solely their loss at the end of training (as considered in \newcite{clark2019dont}). We investigate whether up-weighting forgettable (hard) examples can help in training more robust models.  

We first extend the results of \newcite{toneva2018empirical} by computing forgetting events in MNLI \cite{williams2017broad}, a natural language inference dataset, and by using different architectures of increasing capacity. The robustness of our models is verified by considering their performance on the recently proposed HANS test set \cite{linzen2019right} which contains linguistically correct inference problems that cannot be solved by simple common heuristics, such as lexical overlap, usually learnt by models trained on the MNLI training set. Our best model achieves better performance on the HANS dataset than both BERT and recently proposed robust models. We also discover some interesting insights on forgettable examples for this dataset which contrast with the observations done by \newcite{toneva2018empirical} in the image setting, and open paths for future investigations.

% Each such occurrence is coined a \emph{forgetting event}.
% The authors show that on various image datasets, the distribution of forgetting events is rather striking: numerous examples are never forgotten -- that is, once properly classified they remain so for the rest of training, while others withstand many a forgetting event.
% Visually inspecting those \emph{forgettable examples} shows they are rather atypical compared to the \emph{unforgettable} ones that are fairly prototypical of a class (\textit{e.g.} an occluded gray plane versus a white plane centered on a bright blue sky).
 


