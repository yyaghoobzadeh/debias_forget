\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{latexsym}
\usepackage{subcaption}
\usepackage{url}
\usepackage{stmaryrd}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{multirow}

\newcounter{notecounter}
\newcommand{\enotesoff}{\long\gdef\enote##1##2{}}
\newcommand{\enoteson}{\long\gdef\enote##1##2{{
\stepcounter{notecounter}
{\large\bf
\hspace{1cm}\arabic{notecounter} $<<<$ ##1: ##2
$>>>$\hspace{1cm}}}}}
\enoteson
\aclfinalcopy
\newcommand\alex[1]{\textcolor{red}{Aless: #1}}
\newcommand\remi[1]{\textcolor{green}{Remi: #1}}
\newcommand\yado[1]{\textcolor{orange}{Yado: #1}}

% \input macros

\def\balancedbert{17,748}
\def\balancedlstm{46,740}
\def\balancedbow{63,390}
\def\bertbase{BERT$_{\scriptsize \textrm{BASE}}$ }
\def\bertlarge{BERT$_{\scriptsize \textrm{LARGE}}$ }

\title{Robust Natural Language Inference Models \\ with Example Forgetting}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Yadollah Yaghoobzadeh, Remi Tachet, T.J. Hazen, Alessandro Sordoni \\
  Microsoft Research, Montr\'eal \\
  \small \texttt{\{yayaghoo,retachet,tj.hazen,alsordon\}@microsoft.com}
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
We investigate whether example forgetting, a recently introduced measure of hardness of examples, can be used to select training examples in order to increase robustness of natural language understanding models in a natural language inference task (MNLI). We analyze forgetting events for natural language inference task. We provide evidence that examples that are most forgotten under simpler NLU models can be used to increase robustness of the recently proposed BERT model, measured by testing a MNLI trained model on HANS, a curated test set that exhibits a shift in distribution compared to the MNLI test set. Moreover, we show that, the ``large'' version of BERT is more robust than its ``base'' version but its robustness can still be improved with our approach.
\end{abstract}

\section{Introduction}
\input{intro}

\section{Methodology}
\input{method}

\section{Evaluation}
\label{sec:eval}
\input{results}


\iffalse
\section{Analysis}
\input{analysis}
\fi


\section{Discussion and Conclusion}
\input{discussion}

\bibliography{bibliography}
\bibliographystyle{acl_natbib}

\appendix
\section{Detailed results on HANS}
\label{sec:detailedresults}
\input{hans_detailed_results.tex}

\end{document}

%TODO:
%- https://dawn.cs.stanford.edu/2019/03/22/glue/ (Signal 4: Dataset Slicing) 

% https://microsoft-my.sharepoint.com/:x:/r/personal/yayaghoo_microsoft_com/_layouts/15/Doc.aspx?sourcedoc=%7B41975454-abff-488b-8f55-c550ad840d9f%7D&action=edit&activeCell=%27compare_baseModels%27!D9&wdInitialSession=c219a303-a524-465a-bdd6-28ac21d1312c&wdRldC=1

% \begin{table}[h]
%     \centering
%     \begin{tabular}{l | l l}
%         Train source & HANS & MNLI \\
%         \hline
%         All & 60.0 & \\
%         BERT forgettables & & \\
%         BiLSTM forgettables                       & 54.0 & \\
%         BiLSTM forgettables + 20\% unforgettables & 57.5 & \\
%         BiLSTM forgettables + 30\% unforgettables & 57.5 & \\
%         BiLSTM forgettables + 50\% unforgettables & 57.5 & \\
%         BiLSTM forgettables + 80\% unforgettables & 57.5 & \\
%         \hline 
%         pretrian all, fine-tune on BERT forgettables & 67.8 & \\
%         pretrian all, fine-tune on BiLSTM forgettables & 69.2 & \\
%         pretrian all, fine-tune on BiLSTM 20\% unforgettables & 61.7 & \\
        
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}

