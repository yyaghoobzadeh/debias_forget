
\paragraph{Curriculum Learning and Sample Weighting}
Curriculum learning is a paradigm that favors learning along a curriculum of examples of increasing difficulty~\citep{bengio2009curriculum}. This general idea has found success in a variety of areas since its introduction~\citep{Kumar10,lee2011learning,schaul2015prioritized}.~\citet{Kumar10} implemented their curriculum by considering easy the examples with a small loss. In our experiments, we empirically validate that unforgettable examples can be safely removed without compromising generalization.~\citet{Zhao2015,conf/icml/KatharopoulosF18} relate sample importance to the norm of its loss gradient with respect to the parameters of the network.~\citet{Fan2017,screenerNet,jiang18mentor} learn a curriculum directly from data in order to minimize the task loss.~\citet{jiang18mentor} also study the robustness of their method in the context of noisy examples. This relates to a rich literature on outlier detection and removal of examples with noisy labels~\citep{john1995robust,brodley1999identifying,sukhbaatar2014training,jiang18mentor}. We will provide evidence that noisy examples rank higher in terms of number of forgetting events. \cite{conf/icml/KohL17} borrow influence functions from robust statistics to evaluate the impact of the training examples on a model's predictions.

\paragraph{Deep Generalization} The study of the generalization properties of deep neural networks when trained by stochastic gradient descent has been the focus of several recent publications~\citep{zhang2016understanding, keskar2016large, Chaudhari2016,Advani2017HighdimensionalDO}. These studies suggest that the generalization error does not depend solely on the complexity of the hypothesis space. For instance, it has been demonstrated that over-parameterized models with many more parameters than training points can still achieve low test error~\citep{huang2017densely,Wang2018} while being complex enough to fit a dataset with completely random labels~\citep{zhang2016understanding}.
A possible explanation for this phenomenon is a form of implicit regularization performed by stochastic gradient descent: deep neural networks trained with SGD have been recently shown to converge to the maximum margin solution in the linearly separable case~\citep{Soudry2017,separable2}. In our work, we provide empirical evidence that generalization can be maintained when removing a substantial portion of the training examples and without restricting the complexity of the hypothesis class. This goes along the support vector interpretation provided by \citet{Soudry2017}.

% \paragraph{Continual Learning} \alex{If Maryia get good results, dump some stuff on continual learning, we'd need also to change the intro to make it a bit more concrete!}.

% \begin{itemize}
%    \item https://arxiv.org/pdf/1801.00904.pdf @\textbf{Remi}, here they apply the stuff they do to reinforcement learning in place of the PER (prioritized experience replay). I think it will be worth thinking about it.
%    \begin{itemize}
%        \item 2016: https://arxiv.org/pdf/1606.04232.pdf hard-removal of examples in the training set in the context of noisy labels. Model obtains low accuracy on MNIST, even lower than removing random subsets of the training set. Their model obtains $~93\%$ test accuracy using $3000$ training samples, whereas we have found that retaining $3000$ random examples for training results in a mean accuracy of $98.02\%$ across $3$ seeds with standard deviation of $0.07$. In comparison, our approach of retaining the most forgotten $3000$ examples results in $98.9\%$ accuracy.
%        \item 1999 https://arxiv.org/pdf/1106.0219.pdf identification of noisy labels , nice discussion about the difference between "noise" (to filter) and "exceptions" (to retain)
% \end{itemize}
% \end{itemize}


\paragraph{Out-of-distribution evaluation datasets}
Standard machine learning datasets splits examples in the same distribution into training and evaluation sets.
In NLP as in other ML applications, to test if ML models generalize beyond their training distribution, a number of datasets have been introduced in recent years.
The test sets are made either automatically using designed grammars \cite{linzen2019right}
or by human annotators \cite{xyz}. 
These datasets evaluate different aspects of generalization, e.g., covariate shift and label imbalance. 
The performance of models on these out-of-distribution test sets usually is far from in-distribution test sets.

\paragraph{Unsupervised generalization to out-of-distribution data}
This has been studied within different areas of ML. 
Domain adaptation techniques are trying to build more robust models against covariate shift.
Unlabeled data from target domains might be used in these scenarios but there are also work 
where there is no assumption about the target domain and the goal is to build a more general model which can work better for any unseen target domain. 
Re-weighting training examples based on some criteria is a common approach to gain more generalization.
This of course depends on our prior criteria for generalization in the task.
In this work, we up-weight some training examples without depending on any priors about the task. 
We up-weight examples where training model is uncertain about them during training, i.e., forgets them as training goes on.


\iffalse
\subsection{Comparing our methodology to related work}
\begin{itemize}
    \item The recent work to tackle the brittleness of MNLI trained models all are built based on the prior knowledge about the biases
    in the training dataset. 
    
    
    Notably, \newcite{clark2019dont} introduce a 2 stage process as:
\begin{quote}
        (1)
train a naive model that makes predictions exclusively based on dataset biases, and (2) train
a robust model as part of an ensemble with the
naive one in order to encourage it to focus on
other patterns in the data that are more likely to
generalize
    \end{quote}
    
    \newcite{he2019unlearn} employs a similar approach and uses a bag-of-word, a hypotheis-only and a manually-built bias models, and fits 
    their target model to the residual of these weak baselines.
    
    \newcite{mahabadi2019simple} takes a very similar approach as in \newcite{clark2019dont}.
    
    In compared to these methods, we do not rely on known biases and prior knowledge about the dataset.
    We do however rely on weaker models to compute a more diverse set of forgettables.

\end{itemize}
\fi