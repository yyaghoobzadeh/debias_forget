
\begin{itemize}
    \item Large pretrained networks are SOTA in NLP,
    pushing performance of several benchmarks further and further causing some overstatements in understanding language; there is a need to evaluate them on out-of-distribution in terms of covariate or label shift to test their generalization and robustness.
    
    \item The NLI task is very popular for these kinds of evaluation: it is easy to model, needs all kinds of linguistic knowledge, (but it's also generated non-natural and humans need to create them)
    
    \item There are a number of evaluation datasets now show-casing failures of SOTA NLI models; these datasets are either adversarial to or have distribution shifts from
    training data. 
    
    \item Types of biases: 
    (i) presence of specific words in hypotheses, for example negation words like ``not'' are correlated with contradiction label. 
    \cite{naik2018stress,gururangan2018annotation}.
    (ii) syntactic heuristics, like lexical word overlap: \newcite{naik2018stress} append ``and true is true'' to hypotheses to  decrease lexical overlap of entailments. HANS~\citep{linzen2019right} change labels from entailment to non-entailment while
    keeping the same word-overlap in a controlled setting.
    HANS data is also plausible and there is selection
    creteria based on frequencies in MNLI train set.
    PAWS is another example.
    Other heuristics: subsequence and constituent. 
    (iii) sentence length \cite{gururangan2018annotation}.
    % (iii) hypothesis features \cite{gururangan2018annotation}. 
    
    
    \item The main indicating heuristics used in NLI is the word overlap between hypothesis and premis. \newcite{naik2018stress} study some misclassified examples and find that word overlap is the major reason (29\% of cases). There is high correlation between large word overlap and entailment \cite{linzen2019right}.
    Here we address how to make models more robust against this heuristic.  
    
    \item Recently, there has been multiple efforts in building more robust NLI models. Mostly they assume to
    have access to the biases or heuristics that the evaluation data is testing and they try to mitigate or down-weight them during training. They design suitable bias  models (e.g., models with hypothesis only) for each known bias (e.g., hypothesis standalone features) and by learning a residual target model they 
    increase on o.o.d test sets while decrease slightly \cite{clark2019dont,he2019unlearn,mahabadi2019simple,utama2020mind} or keep the original data performance \cite{utama2020mind}.
    AFLITE~\citep{bras2020adversarial} does not rely on prior knowledge of heuristics but it hurts the original performance significantly.
    
    \item We here propose a new approach that does not rely 
    on known heuristics. We rely on models' performance on training examples to find harder points and then upweight them by just further tuning of the models only on them after training on full data.
    We propose using forgettable examples to select the hard set. Our evaluation on two datasets (HANS and PAWS) that test syntactic generalization of models, our method gains significant improvements. We lose only slightly on the original data.
    
    \item Our analysis shows that the hard sets contain examples that are different from majority of training examples with respect to the distribution of word overlap. 
    By longer training steps \cite{zhou2020instability} or by fine-tuning only on those sets, the networks cover them better which results in 
   more robustness.
    
    \item We further demonstrate that it is important to include hard examples from the beginning of training; warming up with the easy set blocks the model to learn more robustness from hard ones.
    
    \item Our experiments also show how robustness to HANS
    and PAWS can be increased by larger pretrained networks. 
    
    \item While upweighting hard sets improves results of HANS and PAWS, there are a number of limitations for such methods: (i)
    by upweighting the hard sets, performance on original distribution of data decreases and the resulting model does not generalize better 
    to other evaluation benchmarks neither; (ii) 
    the challenging
    datasets do not contain entailment examples with low word overlap; (iii) These limitations suggest that the features learned during upweighting are not still robust.
    
\end{itemize}


